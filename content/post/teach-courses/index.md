---
title:  ğŸ‘©ğŸ¼â€ğŸ« Math285 midterm review
summary: This is the review note for Math285 Midterm (Spring 2025) by Jiashen Ren.
date: 2025-02-02
math: true
authors:
  - admin
tags:
  - Math285
image:
  caption: 'Embed rich media such as videos and LaTeX math'
show_breadcrumb: true
---

## First Order ODE 

### Approach

To solve $y' = a(t)y + b(t)$ :

1. **Find $A(t)$:** Calculate the integral of $a(t)$:  $A(t) = \int a(t) \, dt$. You can ignore the constant of integration for this step.

2. **Calculate $e^{A(t)}$ and $e^{-A(t)}$:**  Compute the exponential of $A(t)$ and its negative.

3. **Find the Integral Part:** Calculate the integral of  $b(t) \cdot e^{-A(t)}$:  $\int b(t)e^{-A(t)} \, dt$.  Let's call the result of this integral $I(t)$.

4. **Particular Solution:** A particular solution is given by $y_p(t) = e^{A(t)} \cdot I(t)$.

5. **General Solution:** The general solution is $y(t) = c \cdot e^{A(t)} + y_p(t)$, where $c$ is an arbitrary constant.

### Sample

**1** For the solution $y(t)$ of the IVP $y' = (y/t) - 1$, $y(1) = \ln 2$, the value $y(2)$ is equal to:

- [x] 0
- [ ] 1
- [ ] 2
- [ ] $\ln 2$
- [ ] $2\ln 2$

**2**. For the solution $y(t)$ of the IVP $y' = \frac{ty+1}{t^2+1}$, $y(0) = 2$, the value $y(1)$ is equal to:

- [ ] $\sqrt{2}$
- [ ] 2
- [ ] $1+\sqrt{2}$
- [ ] 3
- [x] $1+2\sqrt{2}$

**3**. For the solution $y(t)$ of the IVP $y' = \frac{2y+1}{t}$, $y(1) = 2$, the value $y(2)$ is equal to:

- [ ] 11/2
- [ ] 13/2
- [ ] 15/2
- [ ] 17/2
- [x] 19/2

## Separatble ODEs

### Approach

Given a separable ODE of the form:  $\frac{dy}{dt} = f(t)g(y)$

1. **Separate:** $\frac{dy}{g(y)} = f(t) dt$

2. **Integrate:**
   $\int \frac{1}{g(y)} dy = \int f(t) dt$

   Let $G(y) = \int \frac{1}{g(y)} dy$  and  $F(t) = \int f(t) dt$

   Then, the equation becomes: $G(y) = F(t) + C$  (where $C$ is the integration constant)

3. **Apply Initial Condition $y(t_0) = y_0$:** $G(y_0) = F(t_0) + C$

4. **Solve for $C$:** $C = G(y_0) - F(t_0)$

5. **Specific Solution (Implicit Form):** $G(y) = F(t) + G(y_0) - F(t_0)$

6. **Solve for $y$ (Explicit Form - if possible):**
   If possible, solve $G(y) = F(t) + C$ (or $G(y) = F(t) + G(y_0) - F(t_0)$) for $y$ to get $y = h(t, C)$ or $y = h(t, t_0, y_0)$.

7. **Evaluate $y(t_1)$:**
Substitute $t = t_1$ into the explicit solution $y = h(t, C)$ (or implicit solution and solve for $y$) to find $y(t_1)$.

### Sample

1. **For the solution** $y(t)$ **of the IVP** $y' = y\ln t$, $y(1) = 1$ **the value** $y(e)$ **is equal to**

    - [ ] $e^{-2}$
    - [ ] $e^{-1}$
    - [ ] 1
    - [x] $e$
    - [ ] $e^{2}$

2. **For the solution** $y(t)$ **of the IVP** $y' = y^2e^{-t}$, $y(0) = -1$ **the value** $y(-1)$ **is equal to**

    - [ ] $e$
    - [x] $1/(e-2)$
    - [ ] $e+2$
    - [ ] $1/(e+2)$
    - [ ] $e-2$

3. **For the solution** $y(t)$ **of the IVP** $y' = e^{t-2y}$, $y(0) = 0$ **the value** $y(1)$ **is contained in**

    - [ ] $[0, \frac{1}{2}]$
    - [x] $[\frac{1}{2}, 1]$
    - [ ] $[1, \frac{3}{2}]$
    - [ ] $[\frac{3}{2}, 2]$
    - [ ] $[2, \infty)$

4. **For the solution** $y(t)$ **of the IVP** $y' = (y^2-3)/(ty)$, $y(1) = 2$ **the value** $y(2)$ **is equal to**

    - [ ] $\sqrt{6}$
    - [x] $\sqrt{7}$
    - [ ] $\sqrt{8}$
    - [ ] $3$
    - [ ] $\sqrt{10}$

5. **For the solution** $y(t)$ **of the IVP** $y' = -t(y^2+1)$, $y(0) = 1$ **the value** $y(1)$ **is contained in**

    - [x] $[0, \frac{1}{2}]$
    - [ ] $[\frac{1}{2}, 1]$
    - [ ] $[1, \frac{3}{2}]$
    - [ ] $[\frac{3}{2}, 2]$
    - [ ] $[2, \infty)$

## Exact Differential Equations and Integrating Factors

### Approach

**Goal:** Find integrating factor $\mu$ for ODE $M dx + N dy = 0$

**Exactness Check:** Is $M_y = N_x$?  If yes, ODE is exact.

**Integrating Factor Cases:**

| $\mu$ Type      | Condition for $g$                  | Formula for $g$                     | Case No. | $\mu'(s) = $ Relation             |
| :----------------- | :------------------------------------- | :------------------------------------- | :------- | :------------------------------------ |
| $\mu(x)$           | $\frac{M_y - N_x}{N} = g(x)$       | $\frac{M_y - N_x}{N}$                | â‘         | $\mu'(s) = g(s)\mu(s)$              |
| $\mu(y)$           | $\frac{M_y - N_x}{-M} = g(y)$       | $\frac{M_y - N_x}{-M} = \frac{N_x - M_y}{M}$| â‘¡        | $\mu'(s) = -g(s)\mu(s)$             |
| $\mu(xy)$          | $\frac{M_y - N_x}{N_y - M_x} = g(xy)$      | $\frac{M_y - N_x}{N_y - M_x}$        | â‘¢        | $\mu'(s) = g(s)\mu(s)$              |
| $\mu(\frac{y}{x})$ | $\frac{x^2(M_y - N_x)}{N_y + M_x} = g(\frac{y}{x})$ | $\frac{x^2(M_y - N_x)}{N_y + M_x}$    | â‘£        | $\mu'(s) = -g(s)\mu(s)$             |

**Note:**

*   After finding the correct case and $g(s)$, calculate $\mu(s)$ by solving the $\mu'(s) = \pm g(s)\mu(s)$ relationship which leads to  $\mu(s) = e^{\pm \int g(s) ds}$.
*   Multiply the original ODE by $\mu$ to make it exact.

### Sample

1.  **The ODE** $(y^2 + y) dx - x dy$ **has the integrating factor**

    - [ ] 0
    - [ ] $x^{-1}$
    - [ ] $y^{-1}$
    - [ ] $x^{-2}$
    - [x] $y^{-2}$

2.  **The ODE** $(x^4y^2 - y) dx + (x^2y^4 - x) dy = 0$ **has the integrating factor**

    - [ ] $1/(xy)$
    - [ ] 1
    - [x] $1/(xy)^2$
    - [ ] $1/(xy^2)$
    - [ ] $1/(x^2y)$

3.  **The family of curves** $y = c/x^2, c \in \mathbb{R}$ **satisfies the ODE**

    - [ ] $dy = x^{-2} dx$
    - [ ] $dy = 2x^{-3} dx$
    - [x] $2xy dx + x^2 dy = 0$
    - [ ] $dx = dy$
    - [ ] $2yx^{-3} dx - x^{-2} dy = 0$

4.  **The ODE** $3x dx - (y - 3x^2/y) dy$ **has the integrating factor**

    - [ ] 0
    - [ ] $x$
    - [ ] $y$
    - [ ] $x^2$
    - [x] $y^2$

## Radius of Convergence

### Approach

To find the radius of convergence $R$ of a power series $\sum_{n=0}^{\infty} c_n z^n$:

1. **Find the Coefficients:**
   - Identify the coefficients $c_n$ in your power series. This is the number multiplying $z^n$.  Look for a pattern in these numbers as $n$ changes (0, 1, 2, 3...).
   - **Important for series with "gaps":** If some powers of $z$ are missing (like $z^3, z^5$ are present but $z^2, z^4$ are not), the coefficient for the missing power is **zero**. You need to define $c_n$ for *all* $n \ge 0$.

2. **Calculate $|c_n|^{1/n}$ (or Estimate Its Limit):**
   - Take the absolute value of each coefficient $|c_n|$.
   - Raise it to the power of $1/n$ (which is the same as taking the $n$-th root: $\sqrt[n]{|c_n|}$).
   - Think about what happens to $|c_n|^{1/n}$ as $n$ becomes very large.  What is the limit as $n \to \infty$? Let's call this limit $L$.  *In many simple cases, you can just estimate this limit*.

3. **Find the Limit Superior $L = \limsup_{n \to \infty} (|c_n|^{1/n})$ (More precisely):**
   - For more complicated series, or if the limit from step 2 is not obvious, you might need to find the *limit superior*.  But often for introductory problems, the simple limit from step 2 works.  Assume for now that $L = \lim_{n \to \infty} (|c_n|^{1/n})$ exists.

4. **Calculate Radius $R$:**
   -  Once you have $L$, the radius of convergence $R$ is:
      - If $L > 0$, then $R = \frac{1}{L}$.
      - If $L = 0$, then $R = \infty$ (series converges for all $z$).
      - If $L = \infty$, then $R = 0$ (series only converges for $z = 0$).

**Simplified Steps (Formula Focus):**

1.  **Get $c_n$**: Identify the coefficient of $z^n$.
2.  **Calculate $|c_n|^{1/n}$**: Compute the $n$-th root of the absolute value of $c_n$.
3.  **Find $L = \lim_{n\to\infty} |c_n|^{1/n}$** (or $\limsup$).
4.  **Radius $R$**:
    - $R = 1/L$, if $0 < L < \infty$
    - $R = \infty$, if $L = 0$
    - $R = 0$, if $L = \infty$

### Sample

1. The power series $z + \frac{1}{2}z^2 + \frac{1}{4}z^4 + \frac{1}{8}z^8 + \frac{1}{16}z^{16} + \dots$ has radius of convergence

    - [ ] 0
    - [x] 1
    - [ ] $\sqrt{2}$
    - [ ] 2
    - [ ] $\infty$

2. The power series $\sum_{k=1}^{\infty} 2^{k}z^{k^2}$ has radius of convergence

    - [ ] 0
    - [ ] $\frac{1}{2}$
    - [x] 1
    - [ ] 2
    - [ ] $\infty$

3. The power series $z + z^2 + z^4 + z^8 + z^{16} + \dots$ has radius of convergence

    - [ ] 0
    - [ ] $\frac{1}{2}$
    - [x] 1
    - [ ] 2
    - [ ] $\infty$

## Picard-Iteration

### Approach

**Problem-Solving Approach for Picard-LindelÃ¶f Iterates**

To find $\phi_2(t)$ (the second Picard-LindelÃ¶f iterate) for an Initial Value Problem (IVP) using the iterative formula:

$\phi_{k+1}(t) = y_0 + \int_{0}^{t} f(s, \phi_k(s)) \, ds, \quad k = 0, 1, 2, \dots$

**Steps:**

1. **Identify $f(t, y)$ and $y_0$ from the IVP:**
   For an IVP in the form $y' = f(t, y)$, $y(0) = y_0$,  determine the function $f(t, y)$ and the initial value $y_0$.

2. **Determine the Initial Iteration $\phi_0(t)$:**
   The starting point for the iteration is the constant function based on the initial condition:
   $\phi_0(t) = y_0$

3. **Calculate the First Iteration $\phi_1(t)$ (for k=0 in the formula):**
   Use the formula with $k=0$:
   $\phi_1(t) = y_0 + \int_{0}^{t} f(s, \phi_0(s)) \, ds$
   Substitute $\phi_0(s) = y_0$ into $f(s, \phi_0(s))$, then evaluate the definite integral with respect to $s$ from $0$ to $t$.

4. **Calculate the Second Iteration $\phi_2(t)$ (for k=1 in the formula):**
   Use the formula with $k=1$:
   $\phi_2(t) = y_0 + \int_{0}^{t} f(s, \phi_1(s)) \, ds$
   Substitute the expression you found for $\phi_1(s)$ into $f(s, \phi_1(s))$, then evaluate the definite integral with respect to $s$ from $0$ to $t$.

5. **Select the Correct Option:**
   Compare your calculated expression for $\phi_2(t)$ with the provided multiple-choice options and choose the one that matches.

**Formula-Based Steps Summary:**

1. **Identify:** $f(t,y), y_0$ from $y' = f(t, y), y(0) = y_0$
2. **Set:** $\phi_0(t) = y_0$
3. **Calculate $\phi_1(t)$:**
   $\phi_1(t) = y_0 + \int_{0}^{t} f(s, \phi_0(s)) \, ds$
4. **Calculate $\phi_2(t)$:**
   $\phi_2(t) = y_0 + \int_{0}^{t} f(s, \phi_1(s)) \, ds$
5. **Match:** Compare $\phi_2(t)$ to options.

**Important Note:** The integral is always evaluated from $0$ to $t$, and in each step you're substituting the *previous* iterate into the function $f$. You are iteratively refining an approximation to the solution of the IVP.
### Sample

1. The sequence $\phi_0, \phi_1, \phi_2, \dots$ of Picard-LindelÃ¶f iterates for the IVP $y' = y+t$, $y(0) = -1$ has $\phi_2(t)$ equal to

    - [ ] $\frac{1}{6}t^3$
    - [ ] $-1 - t - \frac{1}{2}t^2$
    - [x] $-1 - t + \frac{1}{6}t^3$
    - [ ] $-1 - t - \frac{1}{2}t^2 + \frac{1}{6}t^3$
    - [ ] $-1 - t$

2. The sequence $\phi_0, \phi_1, \phi_2, \dots$ of Picard-LindelÃ¶f iterates for the IVP $y' = y+2t$, $y(0) = -2$ has $\phi_2(t)$ equal to

    - [ ] $-2 + 2t + \frac{1}{3}t^3$
    - [ ] $-t^2 + \frac{1}{3}t^3$
    - [x] $-2 - 2t + \frac{1}{3}t^3$
    - [ ] $t^2 + \frac{1}{3}t^3$
    - [ ] $1 + t + \frac{3}{2}t^2 + \frac{1}{3}t^3$

3. The sequence $\phi_0, \phi_1, \phi_2, \dots$ of Picard-LindelÃ¶f iterates for the IVP $y' = 2y+2$, $y(0) = 2$ has $\phi_2(t)$ equal to

    - [ ] $2t + 6t^2$
    - [ ] $2t + 5t^2$
    - [x] $2 + 6t + 6t^2$
    - [ ] $2t + 4t^2$
    - [ ] $1 + 4t + 4t^2$

## Matrix Norms

### Approach


For a matrix $A$, the norm of $A$ can be defined in different ways. Two common matrix norms are the Operator Norm ($||A||$) and the Frobenius Norm ($||A||_F$).

**1. Operator Norm ($||A||$) - (Subordinate to Euclidean length)**

* **Definition (using unit vectors parameterized by trigonometric functions):**

   Let $x = \begin{pmatrix} \sin x \\ \cos x \end{pmatrix}$, where $x \in [0, 2\pi]$. This $x$ represents any vector on the unit circle in $\mathbb{R}^2$. The Operator Norm of $A$ is defined as:$||A|| = \max \{||Ax||_2 \}$

   where $||Ax||_2$ is the Euclidean norm (length) of the vector $Ax$.  This means we find the maximum length of the vector $Ax$ when $x$ is a unit vector.

* **Example Calculation for  $A = \begin{pmatrix} \frac{1}{2} & \pm 1 \\ 0 & \frac{1}{2} \end{pmatrix}$ :**

   Using $x = \begin{pmatrix} \sin x \\ \cos x \end{pmatrix}$, calculate $||Ax||_2$:

   $||A|| = \max_{x} \left\{ \left\| \begin{pmatrix} \frac{1}{2} & \pm 1 \\ 0 & \frac{1}{2} \end{pmatrix} \begin{pmatrix} \sin x \\ \cos x \end{pmatrix} \right\|_2 \right\}$

   $ = \max_{x} \left\{ \left\| \begin{pmatrix} \frac{1}{2} \sin x \pm \cos x \\ \frac{1}{2} \cos x \end{pmatrix} \right\|_2 \right\}$

   $ = \max_{x} \left\{ \sqrt{ \left( \frac{1}{2} \sin x \pm \cos x \right)^2 + \left( \frac{1}{2} \cos x \right)^2 } \right\}$

   $ = \sqrt{ \frac{3}{4} + \frac{\sqrt{2}}{2} } = \frac{1 + \sqrt{2}}{2} \approx 1.207 $

**2. Frobenius Norm ($||A||_F$):**

* **Definition:**

   The Frobenius Norm of a matrix $A$ is defined as the square root of the sum of the squares of all its elements:$||A||_F = \sqrt{ \sum_{i,j} |a_{ij}|^2 }$

   This is essentially calculating the Euclidean norm of the matrix treated as a vector.

* **Example Calculation for  $A = \begin{pmatrix} \frac{1}{2} & \pm 1 \\ 0 & \frac{1}{2} \end{pmatrix}$ :**

   $||A||_F = \sqrt{ \left(\frac{1}{2}\right)^2 + (\pm 1)^2 + (0)^2 + \left(\frac{1}{2}\right)^2 }$

   $ = \sqrt{ \frac{1}{4} + 1 + 0 + \frac{1}{4} } $

   $ = \sqrt{ \frac{1}{2} + 1 } = \sqrt{ \frac{3}{2} } = \frac{\sqrt{6}}{2} \approx 1.225 $

### Sample

1. The matrix norm of $\begin{pmatrix} 2 & -3 \\ 3 & 2 \end{pmatrix}$ (subordinate to the Euclidean length on $\mathbb{R}^2$) is contained in the interval

    - [ ] $[1, 2]$
    - [ ] $[2, 3]$
    - [x] $[3, 4]$
    - [ ] $[4, 5]$
    - [ ] $[5, 6]$

2. The matrix norm of $\begin{pmatrix} 1 & -1 \\ 1 & 1 \end{pmatrix}$ (subordinate to the Euclidean length on $\mathbb{R}^2$) is equal to

    - [ ] 0
    - [ ] 1
    - [x] $\sqrt{2}$
    - [ ] 2
    - [ ] 4

3. The matrix norm of $\begin{pmatrix} 2 & 1 \\ 1 & 0 \end{pmatrix}$ (subordinate to the Euclidean length on $\mathbb{R}^2$) is equal to

    - [ ] 0
    - [ ] 1
    - [ ] $\sqrt{2}$
    - [ ] 2
    - [x] $1+\sqrt{2}$

## Phase Line

### **1. Basic Concepts**
- **Autonomous Equation**: A differential equation \( y' = f(y) \) that depends only on the variable \( y \) and does not explicitly contain the independent variable \( t \).
- **Equilibrium Points**: The values of \( y \) that make \( f(y) = 0 \), which are critical points where the solution remains constant.
- **Phase Line**: A representation on the \( y \)-axis that marks equilibrium points and analyzes the sign of \( y' \) in their vicinity to determine the increasing or decreasing trend of solutions.

---

### **2. Steps of Phase Line Analysis**
#### **(1) Find Equilibrium Points**
Solve the equation \( f(y) = 0 \) to obtain all equilibrium points \( y = c_1, c_2, \dots \).

#### **(2) Divide into Intervals**
Divide the \( y \)-axis into several intervals using the equilibrium points as dividers. For example, if equilibrium points are \( y = -2, 0, 2 \), the intervals are:
\[ (-\infty, -2),\ (-2, 0),\ (0, 2),\ (2, +\infty) \]

#### **(3) Analyze the Sign of the Derivative**
In each interval, choose any point \( y_0 \) and substitute it into \( f(y) \) to calculate the sign of \( y' \):
- If \( y' > 0 \), the solution is increasing in this interval (arrow points to the right).
- If \( y' < 0 \), the solution is decreasing in this interval (arrow points to the left).

#### **(4) Determine Stability of Equilibrium Points**
- **Stable**: If arrows on both sides point towards the equilibrium point (e.g., \( \rightarrow \leftarrow \)).
- **Unstable**: If arrows on both sides point away from the equilibrium point (e.g., \( \leftarrow \rightarrow \)).
- **Semi-stable**: If on one side the arrow points towards, and on the other side it points away (e.g., \( \rightarrow \rightarrow \) or \( \leftarrow \leftarrow \)).

---

### **3. Application Examples**
#### **Example 1**: \( y' = y^4 - 4y^2 \), initial value \( y(4.29) = 1 \)
1. **Equilibrium Points**: Solve \( y^4 - 4y^2 = 0 \), yielding \( y = -2, 0, 2 \).
2. **Sign of Derivative in Intervals**:
   - \( y > 2 \): \( y' = (+) \) (increasing, arrow to the right).
   - \( 0 < y < 2 \): \( y' = (-) \) (decreasing, arrow to the left).
   - \( -2 < y < 0 \): \( y' = (+) \) (increasing, arrow to the right).
   - \( y < -2 \): \( y' = (-) \) (decreasing, arrow to the left).
3. **Stability**:
   - \( y = -2 \): Stable (arrows on both sides point towards it).
   - \( y = 0 \): Semi-stable (right arrow points left, left arrow points right).
   - \( y = 2 \): Unstable (arrows on both sides point away).
4. **Solution Trend**: Initial value \( y = 1 \) is in \( (0, 2) \), \( y' < 0 \), solution is decreasing and trends towards \( y = 0 \).
   **Limit**: \(\boxed{0}\)

#### **Example 2**: \( y' = y^3 - 7y + 6 \), initial value \( y(0) = 0 \)
1. **Equilibrium Points**: Solve \( y^3 - 7y + 6 = 0 \), yielding \( y = -3, 1, 2 \).
2. **Sign of Derivative in Intervals**:
   - \( y > 2 \): \( y' = (+) \).
   - \( 1 < y < 2 \): \( y' = (-) \).
   - \( -3 < y < 1 \): \( y' = (+) \).
   - \( y < -3 \): \( y' = (-) \).
3. **Stability**:
   - \( y = -3 \): Unstable.
   - \( y = 1 \): Stable.
   - \( y = 2 \): Unstable.
4. **Solution Trend**: Initial value \( y = 0 \) is in \( (-3, 1) \), \( y' > 0 \), solution is increasing and trends towards \( y = 1 \).
   **Limit**: \(\boxed{1}\)

#### **Example 3**: \( y' = y^4 - 1 \), initial value \( y(2021) = 0 \)
1. **Equilibrium Points**: Solve \( y^4 - 1 = 0 \), yielding \( y = -1, 1 \).
2. **Sign of Derivative in Intervals**:
   - \( y > 1 \): \( y' = (+) \).
   - \( -1 < y < 1 \): \( y' = (-) \).
   - \( y < -1 \): \( y' = (+) \).
3. **Stability**:
   - \( y = -1 \): Stable.
   - \( y = 1 \): Unstable.
4. **Solution Trend**: Initial value \( y = 0 \) is in \( (-1, 1) \), \( y' < 0 \), solution is decreasing and trends towards \( y = -1 \).
   **Limit**: \(\boxed{-1}\)

## Second Order ODE (homogenous and inhomogenous)

**$ay'' + by' + cy = r(x)$**

where $a$, $b$, $c$ are constants, and $r(x)$ is a function of $x$.

### Homogenous Second Order ODEs:  $ay'' + by' + cy = 0$

1. **Write down the Characteristic Equation:  $ar^2 + br + c = 0$**

2. **Solve the Characteristic Equation:**

   â€¢ Find the roots $\alpha_1, \alpha_2$ of this quadratic equation using the formula:
     $\alpha_{1,2} = \dfrac{-r_1 \pm \sqrt{r_1^2 - 4r_0}}{2}$

3. **Check Root Type:**

   A. **Distinct Roots** ($\alpha_1 \neq \alpha_2$, when $r_1^2 \neq 4r_0$)
      1. **Real Distinct Roots**
         - General Solution: $y = C_1 e^{\alpha_1 t} + C_2 e^{\alpha_2 t}$
      
      2. **Complex Conjugate Roots** ($\alpha_{1,2} = \zeta \pm i\beta$)
         - General Solution: $y = e^{\zeta t}(C_1 \cos(\beta t) + C_2 \sin(\beta t))$

   B. **Repeated Roots** ($\alpha_1 = \alpha_2 = \alpha$, when $r_1^2 = 4r_0$)
      - General Solution: $y = C_1 e^{\alpha t} + C_2 t e^{\alpha t}$

**Note:** In all cases, $C_1$ and $C_2$ are arbitrary constants determined by initial conditions.

### Inhomogeneous Second Order ODEs:  $ay'' + by' + cy = r(x)$

This section details the "Method of Undetermined Coefficients" (å¾…å®šç³»æ•°æ³•) to find a particular solution $y_p(x)$ for the inhomogeneous equation.

#### 1. Choosing the Form of $y_p(x)$ - Based on r(x)

The choice of the form of the particular solution $y_p(x)$ depends on the form of the function $r(x)$. Use the following table as a guide:

| Term in r(x)          | Choice for $y_p(x)$                                                 |
| :---------------------- | :--------------------------------------------------------------- |
| $ke^{Î³x}$               | $Ce^{Î³x}$                                                         |
| $kx^n$ (n = 0, 1, ...) | $K_nx^n + K_{n-1}x^{n-1} + ... + Kâ‚x + Kâ‚€$                        |
| $k \cos Ï‰x$             | $K \cos Ï‰x + M \sin Ï‰x$                                       |
| $k \sin Ï‰x$             | $K \cos Ï‰x + M \sin Ï‰x$                                       |
| $ke^{Î±x} \cos Ï‰x$       | $e^{Î±x} (K \cos Ï‰x + M \sin Ï‰x)$                               |
| $ke^{Î±x} \sin Ï‰x$       | $e^{Î±x} (K \cos Ï‰x + M \sin Ï‰x)$                               |

where $k, Î³, n, Ï‰, Î±$ are known constants, and $C, K_i, M$ are undetermined coefficients that need to be determined by substituting $y_p(x)$ into the ODE.

#### 2. Rules for Determining $y_p(x)$

*   **Basic Rule :**
    If $r(x)$ consists of terms listed in the table above, then choose $y_p(x)$ to have the corresponding form from the "Choice for $y_p(x)$" column and solve for the undetermined coefficients.

*   **Modification Rule :**
    If a term in the initially chosen $y_p(x)$ is already a solution to the corresponding homogeneous ODE ($ay'' + by' + cy = 0$), modify the choice for $y_p(x)$ by multiplying it by $x$ (or by $x^s$ if necessary, where $s$ is the smallest positive integer that makes no term in the modified $y_p(x)$ a solution to the homogeneous equation. Usually, multiplication by $x$ or $x^2$ is sufficient).

*   **Sum Rule :**
    If $r(x)$ is a sum of several terms, the particular solution $y_p(x)$ should be taken as the sum of the particular solutions corresponding to each term in $r(x)$, adjusting each component with the Modification Rule if needed.

---
**Basic Rule:** è‹¥ $r(x)$ åŒ…å«åœ¨ä¸Šè¡¨å·¦åˆ—ä¸­ï¼Œé‚£ä¹ˆ $y_p(x)$ å°±å–å³ä¾§å¯¹åº”çš„å½¢å¼ï¼Œä¹‹åä»£å…¥æ±‚ç³»æ•°

**Modification Rule:** è‹¥ä½¿ç”¨Basic Ruleé€‰å–çš„ $y_p(x)$ æ˜¯ï¼ˆ6.1ï¼‰å¯¹åº”çš„é½æ¬¡ODEçš„è§£æ—¶ï¼Œå– $y_p(x)$ ä¸ $x$ çš„ä¹˜ç§¯ç”Ÿæˆæ–°çš„ $y_p(x)$ (è‹¥é½æ¬¡ODEæœ‰é‡æ ¹å¤ã€ ä¸” $y_p(x)$ å°±æ˜¯é‚£ä¸ªæ ¹æ—¶ï¼Œ ä¹˜ä»¥çš„æ˜¯ $x^2$ )

**Sum Rule:** è‹¥ $r(x)$ æ˜¯ä¸Šè¡¨å·¦åˆ—å‡ ç§æƒ…å†µçš„åŠ å’Œï¼Œ é‚£ä¹ˆ $y_p(x)$ ä¹Ÿæ˜¯å³ä¾§å¯¹åº”çš„å‡ ç§æƒ…å†µçš„åŠ å’Œï¼Œ æ³¨æ„ï¼ŒåŠ å’Œå‰æ ¹æ®Modification Ruleå¯¹æ¯é¡¹è¿›è¡Œè°ƒæ•´ã€‚

---

### 3. Example: Find the General Solution of $y'' - y = 16e^{-x}$


**STEP1: Solve the corresponding homogeneous equation y'' - y = 0**.  The characteristic equation is $r^2 - 1 = 0 \Rightarrow r = \pm 1$.  Therefore, the complementary function is

$y_c(x) = c_1e^x + c_2e^{-x}$

**STEP2: According to the table, assume  $y_p(x) = Be^{-x}$**. However, because it duplicates a term from the complementary function, according to the Modification Rule, we need to multiply by an $x$, so assume $y_p(x) = Bxe^{-x}$.

**STEP3: Substitute into the original equation, solve for coefficients**.

$16e^{-x} = y_p'' - y_p = B(x - 2)e^{-x} - (Bxe^{-x}) = -2Be^{-x}$
$\Rightarrow B = -8 \Rightarrow y_p(x) = -8xe^{-x}$

**Final general solution for y(x) is** $y(x) = -8xe^{-x} + c_1e^x + c_2e^{-x}$

---
è¡¥å……ç‚¹ç¨€å¥‡å¤æ€ªçš„çŸ¥è¯†
#### é›¶åŒ–ç®—å­æ³•ï¼ˆMethod of Annihilatorsï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡æ„é€ ä¸€ä¸ªèƒ½â€œé›¶åŒ–â€éé½æ¬¡é¡¹çš„å¾®åˆ†ç®—å­ï¼Œå°†åŸéé½æ¬¡æ–¹ç¨‹è½¬åŒ–ä¸ºé«˜é˜¶é½æ¬¡æ–¹ç¨‹ï¼Œä»è€Œç¡®å®šç‰¹è§£çš„å½¢å¼ã€‚

1. **ç¡®å®šé›¶åŒ–ç®—å­**ï¼š  
   é’ˆå¯¹éé½æ¬¡é¡¹ $f(x)$ çš„ç±»å‹é€‰æ‹©ç›¸åº”çš„å¾®åˆ†ç®—å­ $L$ï¼Œä½¿å¾—$L[f(x)] = 0$ã€‚  
   - è‹¥ $f(x) = P_n(x)e^{\alpha x}$ï¼ˆå…¶ä¸­ $P_n(x)$ ä¸º $n$ æ¬¡å¤šé¡¹å¼ï¼‰ï¼Œåˆ™é›¶åŒ–ç®—å­ä¸º  $(D - \alpha)^{n+1}$ã€‚  
   - ä¾‹å¦‚ï¼šè‹¥ $f(x) = 2xe^x$ï¼Œåˆ™é›¶åŒ–ç®—å­ä¸º  $(D - 1)^2$.
   
2. **æ„é€ é«˜é˜¶é½æ¬¡æ–¹ç¨‹**ï¼š  
   å°†é›¶åŒ–ç®—å­ä½œç”¨äºåŸæ–¹ç¨‹ä¸¤è¾¹ï¼Œå¾—åˆ°æ–°çš„é½æ¬¡æ–¹ç¨‹ï¼š  $(D - 1)^2 \cdot (D^2 - 3D + 2)y = 0$ã€‚  
   å…¶å¯¹åº”çš„ç‰¹å¾æ ¹ä¸ºåŸæ–¹ç¨‹çš„æ ¹ï¼ˆ$\lambda = 1,\,2$ï¼‰ä»¥åŠé›¶åŒ–ç®—å­å¼•å…¥çš„é‡æ ¹ï¼ˆ$\lambda = 1$ ä¸‰é‡æ ¹ï¼‰ã€‚
   
3. **ç¡®å®šç‰¹è§£å½¢å¼**ï¼š  
   æ–°é½æ¬¡æ–¹ç¨‹çš„é€šè§£å½¢å¼ä¸ºï¼š$y = (C_1 + C_2x + C_3x^2)e^x + C_4e^{2x}$ã€‚  
   ä»ä¸­é€‰å–ä¸åŸé½æ¬¡è§£  $y_h = C_1e^x + C_2e^{2x}$  
   çº¿æ€§æ— å…³çš„éƒ¨åˆ†ä½œä¸ºç‰¹è§£å½¢å¼ï¼š  $y^* = x(Ax + B)e^x = Ax^2e^x + Bxe^x$.
   
4. **ä»£å…¥åŸæ–¹ç¨‹æ±‚ç³»æ•°**ï¼š  
   å°† $y^*$ ä»£å…¥åŸæ–¹ç¨‹  $y'' - 3y' + 2y = 2xe^x$ï¼Œ  é€šè¿‡æ¯”è¾ƒç³»æ•°è§£å‡º  $A = -2,\quad B = -1$ï¼Œ  
   æœ€ç»ˆç‰¹è§£ä¸ºï¼š$y^* = -2x^2e^x - xe^x$.

#### å¾®åˆ†ç®—å­æ³•(used in final exam)
 ç®—äº†å†™finalé‡Œé¢äº†
---

## Euler Equation

### Approach

**Equation Form:**  $ax^2y'' + bxy' + cy = 0$

**Steps:**

1. **Characteristic Equation:** Substitute $y = x^r$ and derive the characteristic equation:
   $ar(r-1) + br + c = 0$  which simplifies to  $ar^2 + (b-a)r + c = 0$.

2. **Solve for Roots (r):** Use the quadratic formula to find the roots $r_1, r_2$ of the characteristic equation:
   $r = \frac{-(b-a) \pm \sqrt{(b-a)^2 - 4ac}}{2a}$

3. **General Solution based on Roots:**

   * **Distinct Real Roots ($r_1 \neq r_2$ and real):**
     $y(x) = Ax^{r_1} + Bx^{r_2}$

   * **Repeated Real Roots ($r_1 = r_2 = r$):**
     $y(x) = Ax^{r} + Bx^{r} \ln x$

   * **Complex Conjugate Roots ($r = \alpha \pm i\beta$):**
     $y(x) = x^{\alpha} (A \cos(\beta \ln x) + B \sin(\beta \ln x))$

4. **For Non-Homogeneous Equations ($ax^2y'' + bxy' + cy = f(x)$):**

   * **Homogeneous Solution ($y_h$):** Find the general solution of the associated homogeneous equation (steps 1-3).
   * **Particular Solution ($y_p$):** Find a particular solution $y_p$ for the non-homogeneous equation (e.g., using undetermined coefficients or variation of parameters, guessing form based on $f(x)$).
   * **General Solution ($y$):**  $y(x) = y_h(x) + y_p(x)$

5. **Apply Initial Conditions (if given):** Use initial conditions to determine the constants $A$ and $B$ in the general solution.

### Sample

**1**. For the solution $y: (0, \infty) \to \mathbb{R}$ of the IVP  $t^2 y'' - t y' + y = 1, y(1) = y'(1) = 0$  the value $y(e)$ is equal to

- [ ] $\ln 4$
- [x] 1
- [ ] $-1$
- [ ] $1 + \ln 4$
- [ ] $-1 + \ln 4$

**2**. For the solution $y: (0, +\infty) \to \mathbb{R}$ of the IVP  $t^2 y'' + 2t y' - 2y = 1, y(1) = 0, y'(1) = 1$  the value $y(2)$ is equal to

- [ ] $\frac{13}{8}$
- [ ] 1
- [ ] $\frac{13}{24}$
- [ ] $\frac{19}{8}$
- [x] $\frac{19}{24}$

## How to expand $e^{At}$

### Approach

For a 2x2 matrix $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$:

1. **Calculate $A^2$ in the Form:**  Find scalars $p$ and $q$ such that:
   $A^2 = p I_2 + q A$
   where $I_2 = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$.  (Often, $p = bc-ad$ and $q = a+d$, based on characteristic polynomial properties).

2. **Assume Form for $e^{At}$:**  Assume $e^{At}$ can be written as:
   $e^{At} = c_0(t) I_2 + c_1(t) A$

3. **Set up ODE System for $c_0(t), c_1(t)$:** Differentiate the assumed form and equate coefficients of $I_2$ and $A$ using the relation $A^2 = pI_2 + qA$. You will get a system:
   * $c_0'(t) = p c_1(t)$
   * $c_1'(t) = c_0(t) + q c_1(t)$

4. **Initial Conditions:**  Use $e^{A \cdot 0} = I_2$ to get initial conditions:
   * $c_0(0) = 1$
   * $c_1(0) = 0$

5. **Solve for $c_0(t)$:** Convert the system into a second-order ODE for $c_0(t)$:
   $c_0''(t) - q c_0'(t) - p c_0(t) = 0$
   Solve this ODE with derived initial conditions (e.g., $c_0(0)=1, c_0'(0) = p c_1(0) = 0$).  Let the solution be $c_0(t)$.

6. **Solve for $c_1(t)$:** Use the relation $c_1(t) = \frac{1}{p} c_0'(t)$ (if $p \neq 0$) to find $c_1(t)$ from $c_0(t)$.

7. **Construct $e^{At}$:** Substitute $c_0(t)$ and $c_1(t)$ back into the assumed form:
   $e^{At} = c_0(t) I_2 + c_1(t) A$

### Sample

**$A = \begin{pmatrix} 0 & 6 \\ 1 & 1 \end{pmatrix}$ to find $e^{At}$**

**1. Find $p$ and $q$ such that $A^2 = p I_2 + q A$:**
   We found $A^2 = 6I_2 + A$. So, $p = 6$, $q = 1$.

**2. Solve ODE for $c_0(t)$:**
   Equation: $c_0''(t) - c_0'(t) - 6 c_0(t) = 0$.
   Characteristic roots: $r_1 = 3, r_2 = -2$.
   General solution: $c_0(t) = a_{10} e^{-2t} + a_{20} e^{3t}$.

**3. Find $c_1(t)$:**
   Using $c_1(t) = \frac{1}{6} c_0'(t)$, we get:

   $c_1(t) = -\frac{a_{10}}{3} e^{-2t} + \frac{a_{20}}{2} e^{3t} = a_{11} e^{-2t} + a_{21} e^{3t}$  (where $a_{11} = -a_{10}/3, a_{21} = a_{20}/2$).

**4. Use Initial Conditions to find $a_{10}, a_{20}$ (and thus $a_{11}, a_{21}$):**
   From $c_0(0) = 1$ and $c_1(0) = 0$, we solve for $a_{10}$ and $a_{20}$:
   $a_{10} = \frac{3}{5}$,  $a_{20} = \frac{2}{5}$.
   Then $a_{11} = -\frac{1}{5}$, $a_{21} = \frac{1}{5}$.

**5. Substitute back to find $c_0(t), c_1(t)$:**
   $c_0(t) = \frac{3}{5} e^{-2t} + \frac{2}{5} e^{3t}$

   $c_1(t) = -\frac{1}{5} e^{-2t} + \frac{1}{5} e^{3t}$

**6. Construct $e^{At} = c_0(t) I_2 + c_1(t) A$:**
   $e^{At} = (\frac{3}{5} e^{-2t} + \frac{2}{5} e^{3t}) I_2 + (-\frac{1}{5} e^{-2t} + \frac{1}{5} e^{3t}) A $

   $ = \frac{1}{5} \begin{pmatrix} 3e^{-2t} + 2e^{3t} & 6e^{3t} - 6e^{-2t} \\ e^{3t} - e^{-2t} & 2e^{-2t} + 3e^{3t} \end{pmatrix}$

## Maximal solutions

**1**. For the maximal solutions of $y' = y^5 + y$ satisfying $y(0) = 1$, the interval of definition is of the form:

- [ ] $(a,b)$
- [ ] $[a,b]$
- [ ] $(a,+\infty)$
- [x] $(-\infty,b)$
- [ ] $(-\infty,+\infty)$

   with $a,b \in \mathbb{R}$.


**2**. For the maximal solutions of $y' = y^2 + y$ satisfying $y(0) > 0$, the interval of definition is of the form:

- [ ] $(a,b)$
- [ ] $[a,b]$
- [ ] $(a,+\infty)$
- [x] $(-\infty,b)$
- [ ] $(-\infty,+\infty)$

   with $a,b \in \mathbb{R}$.


**3**. For the maximal solutions of $y' = y^3 + 1$ satisfying $y(0) = 0$, the interval of definition is of the form:

- [ ] $(a,b)$
- [ ] $[a,b]$
- [ ] $(a,+\infty)$
- [x] $(-\infty,b)$
- [ ] $(-\infty,+\infty)$

   with $a,b \in \mathbb{R}$.

![alt text](20fc1153d988efeb24215853fff49ba.jpg)

## Point-wise and Uniform Convergence 

> [Video available on Bilibili](https://player.bilibili.com/player.html?isOutside=true&aid=203596893&bvid=BV1hh41127Js&cid=282192338&p=1)

### Differentiable

**ç¬¬ä¸€é¢˜**

é—®é¢˜æ˜¯æ‰¾åˆ°æœ€å¤§çš„æ•´æ•° $s$ï¼Œä½¿å¾—å‡½æ•° $f_s(x) = \sum_{n=1}^{\infty} \frac{\cos(n^s x)}{n^3}$ åœ¨ $\mathbb{R}$ ä¸Šå¯å¾®ã€‚

ä¸ºäº†åˆ¤æ–­ $f_s(x)$ çš„å¯å¾®æ€§ï¼Œæˆ‘ä»¬å¯¹çº§æ•°è¿›è¡Œé€é¡¹æ±‚å¯¼ï¼š
$f'_s(x) = \frac{d}{dx} \left( \sum_{n=1}^{\infty} \frac{\cos(n^s x)}{n^3} \right) = \sum_{n=1}^{\infty} \frac{d}{dx} \left( \frac{\cos(n^s x)}{n^3} \right) = \sum_{n=1}^{\infty} \frac{-n^s \sin(n^s x)}{n^3} = - \sum_{n=1}^{\infty} \frac{\sin(n^s x)}{n^{3-s}}$

ä¸ºäº†ä¿è¯ $f_s(x)$ åœ¨ $\mathbb{R}$ ä¸Šå¯å¾®ï¼Œæˆ‘ä»¬éœ€è¦å¯¼æ•°çº§æ•° $f'_s(x) = - \sum_{n=1}^{\infty} \frac{\sin(n^s x)}{n^{3-s}}$ åœ¨ $\mathbb{R}$ ä¸Šä¸€è‡´æ”¶æ•›ã€‚
ä½¿ç”¨é­å°”æ–¯ç‰¹æ‹‰æ–¯Måˆ¤åˆ«æ³•ï¼Œæˆ‘ä»¬è€ƒå¯Ÿçº§æ•°é¡¹çš„ç»å¯¹å€¼çš„ä¸Šç•Œï¼š
$\left| \frac{\sin(n^s x)}{n^{3-s}} \right| \le \frac{1}{n^{3-s}}$
ä¸ºäº†ä½¿çº§æ•° $\sum_{n=1}^{\infty} \frac{1}{n^{3-s}}$ æ”¶æ•›ï¼ˆp-çº§æ•°ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ $3-s > 1$ï¼Œå³ $s < 2$ã€‚
å› ä¸ºé—®é¢˜è¦æ±‚æœ€å¤§çš„æ•´æ•° $s$ï¼Œæ»¡è¶³ $s < 2$ çš„æœ€å¤§æ•´æ•°æ˜¯ $s = 1$ã€‚

**ç¬¬äºŒé¢˜**

é¢˜ç›®æ˜¯å¯»æ‰¾ä½¿å¾—å‡½æ•° $f_s(x) = \sum_{n=1}^{\infty} \frac{x \sin(nx)}{n^s+1}$ åœ¨å®æ•° $\mathbb{R}$ ä¸Šå¯å¾®çš„æœ€å°æ•´æ•° $s$ã€‚

1. **å¯å¾®æ€§çš„æ¡ä»¶:**  è¦ä½¿å‡½æ•° $f_s(x)$ å¯å¾®ï¼Œæˆ‘ä»¬éœ€è¦è€ƒå¯Ÿå…¶å¯¼æ•°ã€‚æˆ‘ä»¬å°è¯•å¯¹ $f_s(x)$ è¿›è¡Œé€é¡¹æ±‚å¯¼ã€‚

2. **é€é¡¹æ±‚å¯¼:**  å¯¹æ¯ä¸€é¡¹ $\frac{x \sin(nx)}{n^s+1}$ æ±‚å¯¼ï¼š
   $$ \frac{d}{dx} \left( \frac{x \sin(nx)}{n^s+1} \right) = \frac{1}{n^s+1} \frac{d}{dx} (x \sin(nx)) $$
   ä½¿ç”¨ä¹˜ç§¯æ³•åˆ™æ±‚å¯¼ $x \sin(nx)$ï¼š
   $$ \frac{d}{dx} (x \sin(nx)) = (1) \cdot \sin(nx) + x \cdot \frac{d}{dx} (\sin(nx)) = \sin(nx) + x \cdot (n \cos(nx)) = \sin(nx) + nx \cos(nx) $$
   æ‰€ä»¥ï¼Œå¯¼æ•°é¡¹ä¸ºï¼š
   $$ \frac{d}{dx} \left( \frac{x \sin(nx)}{n^s+1} \right) = \frac{\sin(nx) + nx \cos(nx)}{n^s+1} = \frac{\sin(nx)}{n^s+1} + \frac{nx \cos(nx)}{n^s+1} $$

3. **å¯¼æ•°çº§æ•°:** é‚£ä¹ˆï¼Œå‡½æ•° $f_s(x)$ çš„å¯¼æ•° $f'_s(x)$ çš„å½¢å¼å°†æ˜¯å¯¼æ•°é¡¹çš„çº§æ•°å’Œï¼š
   $$ f'_s(x) = \sum_{n=1}^{\infty} \left( \frac{\sin(nx)}{n^s+1} + \frac{nx \cos(nx)}{n^s+1} \right) = \sum_{n=1}^{\infty} \frac{\sin(nx)}{n^s+1} + \sum_{n=1}^{\infty} \frac{nx \cos(nx)}{n^s+1} $$

4. **å¯¼æ•°çº§æ•°çš„æ”¶æ•›æ€§:** ä¸ºäº†ä¿è¯ $f_s(x)$ åœ¨ $\mathbb{R}$ ä¸Šå¯å¾®ï¼Œæˆ‘ä»¬éœ€è¦ $f'_s(x)$ çš„çº§æ•°åœ¨ $\mathbb{R}$ ä¸Šæ”¶æ•›ã€‚æˆ‘ä»¬åˆ†åˆ«è€ƒå¯Ÿè¿™ä¸¤ä¸ªçº§æ•°çš„æ”¶æ•›æ€§ã€‚

   * **ç¬¬ä¸€ä¸ªçº§æ•°:  $\sum_{n=1}^{\infty} \frac{\sin(nx)}{n^s+1}$**
      å¯¹äºè¿™ä¸ªçº§æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨é­å°”æ–¯ç‰¹æ‹‰æ–¯Måˆ¤åˆ«æ³•è¿›è¡Œåˆ†æã€‚
      $$ \left| \frac{\sin(nx)}{n^s+1} \right| \le \frac{1}{n^s+1} $$
      ä¸ºäº†ä½¿ $\sum_{n=1}^{\infty} \frac{1}{n^s+1}$ æ”¶æ•›ï¼ˆp-çº§æ•°åˆ¤åˆ«æ³•çš„å˜ä½“ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ $s > 0$ã€‚

   * **ç¬¬äºŒä¸ªçº§æ•°:  $\sum_{n=1}^{\infty} \frac{nx \cos(nx)}{n^s+1}$**
      å¯¹äºè¿™ä¸ªçº§æ•°ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ $x$ çš„å½±å“ã€‚ä¸ºäº†ä½¿çº§æ•°å¯¹æ‰€æœ‰ $x \in \mathbb{R}$ éƒ½æ”¶æ•›ï¼Œæˆ‘ä»¬è€ƒè™‘å…¶ç»å¯¹å€¼çš„ä¸Šç•Œã€‚
      $$ \left| \frac{nx \cos(nx)}{n^s+1} \right| = \frac{|x| \cdot n \cdot |\cos(nx)|}{n^s+1} \le \frac{|x| \cdot n}{n^s+1} $$
      ä¸ºäº†ä½¿ $\sum_{n=1}^{\infty} \frac{|x| \cdot n}{n^s+1}$ æ”¶æ•›ï¼Œå¯¹äºå›ºå®šçš„ $x$ï¼Œæˆ‘ä»¬éœ€è¦çœ‹ $n$ çš„å¹‚æ¬¡ã€‚å½“ $n$ å¾ˆå¤§æ—¶ï¼Œ$\frac{|x| \cdot n}{n^s+1} \approx \frac{|x| \cdot n}{n^s} = \frac{|x|}{n^{s-1}}$ã€‚
      ä¸ºäº†ä¿è¯çº§æ•° $\sum_{n=1}^{\infty} \frac{|x|}{n^{s-1}}$ æ”¶æ•›ï¼ˆp-çº§æ•°ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ $s-1 > 1$ï¼Œå³ $s > 2$ã€‚

5. **ç¡®å®šæœ€å°æ•´æ•° s:**  ä¸ºäº†ä½¿ $f'_s(x)$ çš„ä¸¤ä¸ªéƒ¨åˆ†éƒ½æ”¶æ•›ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶æ»¡è¶³ $s > 0$ å’Œ $s > 2$ã€‚ æ›´ä¸¥æ ¼çš„æ¡ä»¶æ˜¯ $s > 2$ã€‚ å› æ­¤ï¼Œæ»¡è¶³ $s > 2$ çš„æœ€å°æ•´æ•°æ˜¯ $s = 3$ã€‚

6. **éªŒè¯ s=3 çš„æƒ…å†µ:** å½“ $s=3$ æ—¶ï¼Œå¯¼æ•°çº§æ•°å˜ä¸º $\sum_{n=1}^{\infty} \left( \frac{\sin(nx)}{n^3+1} + \frac{nx \cos(nx)}{n^3+1} \right)$ã€‚ å¯¹äº $s=3$ï¼Œæˆ‘ä»¬å·²ç»åˆ†æè¿‡ï¼Œè¿™ä¸¤ä¸ªçº§æ•°éƒ¨åˆ†éƒ½ä¸€è‡´æ”¶æ•›ï¼ˆé€šè¿‡é­å°”æ–¯ç‰¹æ‹‰æ–¯Måˆ¤åˆ«æ³•ï¼Œå¹¶è€ƒè™‘åˆ° $|x|$ çš„å½±å“ï¼Œè™½ç„¶æ›´ä¸¥è°¨çš„éœ€è¦è€ƒè™‘å±€éƒ¨ä¸€è‡´æ”¶æ•›æˆ–è€…å¯¹äºå›ºå®šçš„ x è®¨è®ºï¼Œä½†ä¸ºäº†å¾—åˆ°æœ€å°æ•´æ•°ï¼Œæ­¤å¤„çš„åˆ†æè¶³å¤Ÿåˆ¤æ–­ï¼‰ã€‚ å› æ­¤ï¼Œå½“ $s=3$ æ—¶ï¼Œ$f_s(x)$ æ˜¯å¯å¾®çš„ã€‚

---
$$\sum_{n=1}^{\infty} \frac{\cos(nx)}{n} = -\ln \left(2 \sin \frac{x}{2} \right)$$

$$\sum_{n=1}^{\infty} \frac{\sin(nx)}{n} = \frac{\pi - x}{2} \quad (0 < x < 2\pi)$$

---

### Function Sequence Converge Uniformly

#### Definition

**I. Understanding Definitions:**

1. **Pointwise Convergence (Point-wise Convergence):**

   * **Definition:** A function sequence $(f_n)$ converges pointwise on an interval $I$ if, for **every fixed point** $x \in I$, the sequence of function values $(f_n(x))$ (which becomes an ordinary sequence of real numbers) converges.
   * **Limit Function:** If pointwise convergence occurs, we can define a **limit function** $f: I \to \mathbb{R}$, where $f(x) = \lim_{n\to\infty} f_n(x)$. This $f(x)$ is called the "limit function" or "pointwise limit" of the sequence $(f_n)$.
   * **Key Point:** Convergence is considered **independently for each point** $x$. For different $x$, the sequence may converge at different rates, and even the choice of $N$ might depend on $x$.

2. **Uniform Convergence (Uniform Convergence):**

   * **Definition:** A function sequence $(f_n)$ converges uniformly on an interval $I$ if it is first **pointwise convergent**, and its limit function $f: I \to \mathbb{R}$ must also satisfy the following property:
      * **Uniform Response N:** For **every** given tolerance of error $\epsilon > 0$, **there exists a "uniform response" positive integer $N \in \mathbb{N}$ (depending only on $\epsilon$, not on $x$)**, such that for **all** $n > N$ and **all** $x \in I$, the inequality $|f(x) - f_n(x)| < \epsilon$ holds.
   * **Key Point:** The key to "uniform" is that **the choice of $N$ is independent of $x$**. This means that for a given accuracy requirement $\epsilon$, we can find a common $N$, from which point onwards, all functions $f_n(x)$ (for $n > N$) will be "sufficiently close" to the limit function $f(x)$ across the entire interval $I$, with the error being less than $\epsilon$.

**II. Methodology Steps for Checking Uniform Convergence of a Function Sequence:**

To determine whether a function sequence $(f_n)$ converges uniformly on an interval $I$, you can follow these steps:

1. **Find the Pointwise Limit Function:**
   * For each fixed $x \in I$, calculate the limit $\lim_{n\to\infty} f_n(x) = f(x)$.
   * Determine the pointwise limit function $f(x)$. If for some $x \in I$, the limit does not exist or is not a finite value, then the function sequence cannot converge uniformly to a finite function on $I$.

2. **Calculate the Error Function:**
   * Define the error function (or remainder term) $e_n(x) = f_n(x) - f(x)$, or consider its absolute value $|e_n(x)| = |f_n(x) - f(x)|$.

3. **Find the Supremum (Uniform Norm) of the Error Function:**
   * Find the supremum (or maximum, if it exists) of the absolute value of the error function on the interval $I$:
      $$M_n = \sup_{x \in I} |f_n(x) - f(x)|$$
   * This step may require using calculus methods (finding derivatives to find extrema, considering interval endpoints, etc.) or using inequalities to estimate an upper bound.

4. **Determine the Limit of the Supremum:**
   * Calculate the limit of the sequence $\{M_n\}$ as $n \to \infty$: $\lim_{n\to\infty} M_n = L$.

5. **Draw a Conclusion:**
   * **If $L = 0$:** Then the function sequence $(f_n)$ converges **uniformly** on the interval $I$ to the limit function $f(x)$.
   * **If $L > 0$ or the limit does not exist (and is not 0):** Then the function sequence $(f_n)$ is **not uniformly convergent** on the interval $I$ to the limit function $f(x)$.

#### ç¢ç¢å¿µç‰ˆ

**æƒ³è±¡ä¸€ä¸‹ä½ æœ‰ä¸€æ’å‡½æ•°ï¼Œå°±åƒä¸€é˜Ÿè¿åŠ¨å‘˜è¦è·‘å‘ç»ˆç‚¹çº¿ï¼ˆæé™å‡½æ•°ï¼‰ã€‚**

**1. é€ç‚¹æ”¶æ•› (Point-wise Convergence) å°±åƒï¼šæ¯ä¸ªäºº *å•ç‹¬* åˆ°è¾¾ç»ˆç‚¹çº¿ã€‚**

* ä½ çœ‹è·‘é“ä¸Šçš„ *æ¯ä¸ªä½ç½®* (æ¯ä¸ªç‚¹ $x$)ï¼š  è¿åŠ¨å‘˜åœ¨è¿™ä¸ªä½ç½®ä¸Šçš„é€Ÿåº¦ï¼ˆå‡½æ•°å€¼ $f_n(x)$ï¼‰éšç€è®­ç»ƒï¼ˆ$n$ å¢å¤§ï¼‰éƒ½åœ¨ *é€æ¸* æ¥è¿‘æŸä¸ªå›ºå®šé€Ÿåº¦ï¼ˆæé™å€¼ $f(x)$ï¼‰ã€‚
* æ¯ä¸ªäºº *å„è‡ªåŠªåŠ›* åˆ°è¾¾ç»ˆç‚¹çº¿ã€‚æœ‰çš„è¿åŠ¨å‘˜å¯èƒ½ä¸€å¼€å§‹è·‘å¾—æ…¢ï¼Œä½†åªè¦æœ€ç»ˆèƒ½åˆ°è¾¾ï¼Œå°±ç®—é€ç‚¹æ”¶æ•›äº†ã€‚
* **ç¼ºç‚¹:** è™½ç„¶æ¯ä¸ªä½ç½®æœ€ç»ˆéƒ½æ¥è¿‘äº†ç›®æ ‡ï¼Œä½†æ˜¯ *å¤§å®¶åˆ°è¾¾ç»ˆç‚¹çš„é€Ÿåº¦å¯èƒ½ä¸ä¸€æ ·*ï¼Œæœ‰çš„ä½ç½®å¯èƒ½å¿«ï¼Œæœ‰çš„ä½ç½®å¯èƒ½æ…¢ã€‚

**2. ä¸€è‡´æ”¶æ•› (Uniform Convergence) å°±åƒï¼šæ‰€æœ‰äºº *é½æ­¥èµ°*ï¼Œ *æ•´é½åˆ’ä¸€* åœ°åˆ°è¾¾ç»ˆç‚¹çº¿ã€‚**

* ä¸ä»…è¦æ¯ä¸ªäººéƒ½åˆ°ç»ˆç‚¹ï¼Œè¿˜è¦ *å¤§å®¶æ­¥è°ƒä¸€è‡´*ã€‚
* ä½ è®¾å®šä¸€ä¸ªè¯¯å·®èŒƒå›´ï¼Œæ¯”å¦‚â€œå¤§å®¶ç¦»ç»ˆç‚¹çº¿çš„è·ç¦»ä¸èƒ½è¶…è¿‡ 1 ç±³â€ã€‚ **ä¸€è‡´æ”¶æ•›è¦æ±‚ï¼Œå¿…é¡»è¦æœ‰ä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒè®¡åˆ’ï¼ˆä¸€ä¸ªå…±åŒçš„ $N$ï¼‰ï¼Œä½¿å¾—è®­ç»ƒåˆ°ä¸€å®šç¨‹åº¦åï¼Œ*æ‰€æœ‰äºº* åœ¨ *ä»»ä½•æ—¶åˆ»*ï¼Œç¦»ç»ˆç‚¹çº¿çš„è·ç¦»éƒ½å°äºä½ è®¾å®šçš„è¯¯å·®èŒƒå›´ã€‚**
* **ä¼˜ç‚¹:**  ä¸€è‡´æ”¶æ•›æ¯”é€ç‚¹æ”¶æ•›æ›´ â€œæ•´é½â€ï¼Œæ›´â€œå¥½â€ã€‚ å®ƒä¿è¯äº†æ•´ä¸ªå‡½æ•°åºåˆ— â€œå‡åŒ€åœ°â€ é è¿‘æé™å‡½æ•°ã€‚

**ç”¨æ›´ç®€å•çš„æ­¥éª¤æ¥è¯´æ˜æ£€éªŒä¸€è‡´æ”¶æ•›çš„æ–¹æ³•ï¼š**

1. **å…ˆçœ‹çœ‹æ¯ä¸ªäºº *æœ€ç»ˆæƒ³è·‘åˆ°å“ªé‡Œ* (æ±‚é€ç‚¹æé™å‡½æ•° $f(x)$)ã€‚**  åœ¨è·‘é“çš„æ¯ä¸ªä½ç½® $x$ï¼Œå‡½æ•°å€¼ $f_n(x)$ æœ€åä¼šæ¥è¿‘å“ªä¸ªæ•° $f(x)$ï¼Ÿ

2. **ç®—ç®—æ¯ä¸ªäºº *ç¦»ç›®æ ‡çš„è·ç¦»* æœ‰å¤šè¿œ (è¯¯å·®å‡½æ•° $|f_n(x) - f(x)|$).**  å¯¹äºæ¯ä¸ªè®­ç»ƒé˜¶æ®µ $n$ï¼Œåœ¨è·‘é“çš„ *æ¯ä¸ªä½ç½®* $x$ï¼Œç›®å‰çš„å‡½æ•°å€¼ $f_n(x)$ å’Œæœ€ç»ˆç›®æ ‡ $f(x)$ å·®å¤šå°‘ï¼Ÿ

3. **æ‰¾åˆ° *æœ€å¤§* çš„è·ç¦» (ä¸€è‡´èŒƒæ•° $\sup_{x \in I} |f_n(x) - f(x)|$).** åœ¨è®­ç»ƒé˜¶æ®µ $n$ï¼Œåœ¨ *æ•´ä¸ªè·‘é“* ä¸Šï¼Œè¿åŠ¨å‘˜ç¦»ç›®æ ‡ *æœ€è¿œ* èƒ½æœ‰å¤šè¿œï¼Ÿ  æˆ‘ä»¬è¦æ‰¾çš„æ˜¯æœ€ç³Ÿç³•æƒ…å†µä¸‹çš„è¯¯å·®ã€‚

4. **çœ‹çœ‹ *æœ€å¤§è·ç¦»* ä¼šä¸ä¼š *è¶Šæ¥è¶Šå°* (åˆ¤æ–­ $\lim_{n\to\infty} M_n = 0$).**  éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè¿™ä¸ª *æœ€å¤§è·ç¦»* (æœ€åæƒ…å†µä¸‹çš„è¯¯å·®) ä¼šä¸ä¼šé€æ¸ç¼©å°åˆ° 0ï¼Ÿ  å¦‚æœä¼šï¼Œé‚£å°±æ˜¯ä¸€è‡´æ”¶æ•›ï¼›å¦‚æœä¸ä¼šï¼Œå°±ä¸æ˜¯ã€‚

**è®°ä½å…³é”®ç‚¹ï¼š**

* **é€ç‚¹æ”¶æ•›æ˜¯ *å±€éƒ¨* çš„ï¼š**  åªçœ‹æ¯ä¸ªç‚¹ã€‚
* **ä¸€è‡´æ”¶æ•›æ˜¯ *å…¨å±€* çš„ï¼š**  çœ‹æ•´ä¸ªåŒºé—´ï¼Œçœ‹æœ€åæƒ…å†µã€‚

**ç®€å•ç²—æš´åœ°è¯´ï¼š**

* **ä¸€è‡´æ”¶æ•›  =  é€ç‚¹æ”¶æ•› +  â€œæ•´é½åˆ’ä¸€â€ çš„é è¿‘æé™å‡½æ•°ã€‚**

#### Sample Questions

**8. å¯¹äº $f_n(x)$ çš„å“ªä¸ªé€‰æ‹©ï¼Œå‡½æ•°åºåˆ— $(f_n)$ åœ¨åŒºé—´ $(0, 1)$ ä¸Šä¸€è‡´æ”¶æ•›ï¼Ÿ**

**é€‰é¡¹:**

* (A) $\frac{\ln x}{n}$
* (B) $n \sqrt{x}$
* (C) $\frac{nx}{x+n}$
* (D) $e^{-nx^2}$
* (E) $\frac{x}{n} \ln \frac{x}{n}$

**æ­£ç¡®ç­”æ¡ˆæ˜¯ (C) å’Œ (E).**

**è§£é‡Š:**

**åœ¨ (A) ä¸­ï¼Œæé™å‡½æ•°æ˜¯å½“ $x \to 0$ æ—¶ï¼Œä½†å¯¹äº $n=1, 2, \ldots$ï¼Œæˆ‘ä»¬æœ‰ $f_n(e^{-n}) = -1$ï¼Œè¿™è¡¨æ˜å¯¹äº $\epsilon = 1$ (ä»¥åŠæ›´å°çš„ $\epsilon$ å€¼) ä¸å¯èƒ½æ‰¾åˆ°ä¸€è‡´å“åº”ï¼ˆuniform responseï¼Œè¿™é‡ŒæŒ‡å¯¹äºæ‰€æœ‰ $x$ å’Œè¶³å¤Ÿå¤§çš„ $n$ï¼Œè¯¯å·®éƒ½å°äº $\epsilon$ çš„ $N$ï¼‰ã€‚**

* **è¯¦ç»†è§£é‡Š (A):**
    * **é€ç‚¹æé™:**  å¯¹äºä»»æ„ $x \in (0, 1)$ï¼Œ$\lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \frac{\ln x}{n} = 0$ (å› ä¸º $\ln x$ å¯¹äºå›ºå®šçš„ $x \in (0, 1)$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œè€Œåˆ†æ¯ $n$ è¶‹å‘äºæ— ç©·å¤§)ã€‚  é€ç‚¹æé™å‡½æ•° $f(x) = 0$ã€‚
    * **éä¸€è‡´æ”¶æ•›:** ä¸ºäº†æ£€éªŒæ˜¯å¦ä¸€è‡´æ”¶æ•›ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥çœ‹ $\sup_{x \in (0, 1)} |f_n(x) - f(x)| = \sup_{x \in (0, 1)} |\frac{\ln x}{n}|$ æ˜¯å¦è¶‹å‘äº 0 å½“ $n \to \infty$ã€‚
    * å› ä¸ºå½“ $x \to 0^+$ æ—¶ï¼Œ$\ln x \to -\infty$ï¼Œæ‰€ä»¥ $|\ln x| \to \infty$ã€‚  å¯¹äºä»»ä½•å›ºå®šçš„ $n$ï¼Œ$\sup_{x \in (0, 1)} |\frac{\ln x}{n}| = \sup_{x \in (0, 1)} \frac{-\ln x}{n} = \infty$ (å› ä¸º $-\ln x$ å¯ä»¥æ— é™å¤§)ã€‚
    * å› æ­¤ï¼Œä¸€è‡´èŒƒæ•°ä¸è¶‹äº 0ï¼Œæ”¶æ•›ä¸æ˜¯ä¸€è‡´çš„ã€‚
    * è§£é‡Šä¸­ç”¨ $x = e^{-n}$ ä½œä¸ºä¾‹å­ã€‚å½“ $x = e^{-n}$ æ—¶ï¼Œ $f_n(e^{-n}) = \frac{\ln(e^{-n})}{n} = \frac{-n}{n} = -1$ã€‚  è¿™æ„å‘³ç€å­˜åœ¨æŸäº› $x$ å€¼ï¼ˆä¾‹å¦‚ $x = e^{-n}$ï¼‰ï¼Œä½¿å¾— $|f_n(x) - 0| = |-1 - 0| = 1$ï¼Œæ— è®º $n$ å¤šå¤§ï¼Œè¯¯å·®éƒ½ä¿æŒåœ¨ 1ï¼Œä¸è¶‹è¿‘äº 0ï¼Œæ‰€ä»¥ä¸æ˜¯ä¸€è‡´æ”¶æ•›ã€‚

**åœ¨ (B) ä¸­ï¼Œæé™å‡½æ•°æ˜¯å½“ $x \to 1$ æ—¶ï¼Œä½†å¯¹äº $n=1, 2, \ldots$ï¼Œæˆ‘ä»¬æœ‰ $f_n(1/n^2) = 1/2$ for $n=1, 2, \ldots$ï¼Œè¿™è¡¨æ˜å¯¹äº $\epsilon = 1/2$ ä¸å¯èƒ½æ‰¾åˆ°ä¸€è‡´å“åº”ã€‚**

* **è¯¦ç»†è§£é‡Š (B):**
    * **é€ç‚¹æé™:**  å¯¹äºä»»æ„ $x \in (0, 1)$ï¼Œ$\lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} n \sqrt{x} = \infty$ (å½“ $x > 0$ æ—¶)ã€‚ å‡½æ•°åºåˆ—åœ¨ $(0, 1)$ ä¸Šä¸é€ç‚¹æ”¶æ•›åˆ°æœ‰é™å‡½æ•°ã€‚
    * **éä¸€è‡´æ”¶æ•›:** ç”±äºå‡½æ•°åºåˆ—ä¸é€ç‚¹æ”¶æ•›åˆ°æœ‰é™å‡½æ•°ï¼Œè‡ªç„¶ä¹Ÿä¸å¯èƒ½ä¸€è‡´æ”¶æ•›åˆ°æœ‰é™å‡½æ•°ã€‚
    * è§£é‡Šä¸­ç”¨äº† $x = 1/n^2$ã€‚ å½“ $x = 1/n^2$ æ—¶ (æ³¨æ„è¿™é‡Œè§£é‡Šä¸­å†™çš„æ˜¯ $1/2^n$ ä¼¼ä¹æœ‰è¯¯ï¼Œåº”è¯¥æ˜¯ $1/n^2$ æ›´åˆç†)ï¼Œ $f_n(1/n^2) = n \sqrt{1/n^2} = n \cdot (1/n) = 1$ã€‚  å¦‚æœåƒè§£é‡Šä¸­å†™çš„æ˜¯ $1/2^{n^2}$ï¼Œé‚£å½“ $x=1/2^{n^2}$ æ—¶ï¼Œ$f_n(1/2^{n^2}) = n \sqrt{1/2^{n^2}} = n / 2^n \to 0$ as $n \to \infty$ï¼Œ è¿™ä¼¼ä¹åè€Œæš—ç¤ºäº†æ”¶æ•›ï¼Œä¸è§£é‡ŠçŸ›ç›¾ã€‚ åº”è¯¥æ˜¯è§£é‡Šæˆ–é¢˜ç›®ä¸­çš„å‡½æ•°æœ‰ç¬”è¯¯ã€‚ **æ›´æ­£ï¼š** å¦‚æœè§£é‡Šä¸­å†™çš„æ˜¯ $f_n(1/2^{n^2}) = 1/2$ å¯èƒ½ä¹Ÿæ˜¯ç¬”è¯¯ï¼Œä¹Ÿè®¸æƒ³è¡¨è¾¾çš„æ˜¯æŸä¸ªé”™è¯¯è®¡ç®—ï¼Œæˆ–è€…åŸå§‹é—®é¢˜å¯èƒ½å­˜åœ¨å°åˆ·é”™è¯¯ã€‚ **å…³é”®ç†è§£ï¼š** å‡½æ•° $n\sqrt{x}$ å½“ $n \to \infty$ æ—¶ï¼Œå¯¹äº $x > 0$ è¶‹äºæ— ç©·å¤§ï¼Œæ‰€ä»¥ä¸å¯èƒ½ä¸€è‡´æ”¶æ•›åˆ°æœ‰é™å‡½æ•°ã€‚

**åœ¨ (C) ä¸­ï¼Œæé™å‡½æ•°æ˜¯ $x \to x$ï¼Œæˆ‘ä»¬æœ‰**

$$\left| \frac{nx}{x+n} - x \right| = \left| \frac{-x^2}{x+n} \right| = \frac{x^2}{x+n} \le \frac{1}{n} \quad \text{å¯¹äº } 0 < x < 1,$$

**æš—ç¤ºäº†ä¸€è‡´æ”¶æ•›æ€§ã€‚ï¼ˆä½œä¸ºå¯¹ $\epsilon > 0$ çš„ä¸€è‡´å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥å– $N = \lceil 1/\epsilon \rceil$.ï¼‰**

* **è¯¦ç»†è§£é‡Š (C):**
    * **é€ç‚¹æé™:**  å¯¹äºä»»æ„ $x \in (0, 1)$ï¼Œ$\lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \frac{nx}{x+n} = \lim_{n \to \infty} \frac{x}{x/n + 1} = x$ã€‚ é€ç‚¹æé™å‡½æ•° $f(x) = x$ã€‚
    * **ä¸€è‡´æ”¶æ•›:**  æˆ‘ä»¬éœ€è¦æŸ¥çœ‹ $\sup_{x \in (0, 1)} |f_n(x) - f(x)| = \sup_{x \in (0, 1)} |\frac{nx}{x+n} - x| = \sup_{x \in (0, 1)} \frac{x^2}{x+n}$ æ˜¯å¦è¶‹å‘äº 0 å½“ $n \to \infty$ã€‚
    * è§£é‡Šä¸­ç»™å‡ºä¸ç­‰å¼ $\frac{x^2}{x+n} \le \frac{1}{n}$ã€‚ è¿™æ˜¯å› ä¸ºå¯¹äº $x \in (0, 1)$ å’Œ $n \ge 1$,  $x+n \ge n$, æ‰€ä»¥ $\frac{x^2}{x+n} \le \frac{x^2}{n}$. åˆå› ä¸º $x \in (0, 1)$, æ‰€ä»¥ $x^2 < 1$, æ•… $\frac{x^2}{n} < \frac{1}{n}$.  æ›´ç›´æ¥çš„ä¼°è®¡æ˜¯å¯¹äº $x \in (0, 1)$, $x^2 < 1$, $x+n > n$, æ‰€ä»¥ $\frac{x^2}{x+n} < \frac{1}{n}$.
    * è¿™æ · $\sup_{x \in (0, 1)} |f_n(x) - f(x)| = \sup_{x \in (0, 1)} \frac{x^2}{x+n} \le \frac{1}{n}$.
    * ç”±äº $\frac{1}{n} \to 0$ å½“ $n \to \infty$ï¼Œæ‰€ä»¥ä¸€è‡´èŒƒæ•°è¶‹äº 0ï¼Œå› æ­¤å‡½æ•°åºåˆ—ä¸€è‡´æ”¶æ•›ã€‚
    * è§£é‡Šä¸­æåˆ° "ä½œä¸ºå¯¹ $\epsilon > 0$ çš„ä¸€è‡´å“åº”ï¼Œæˆ‘ä»¬å¯ä»¥å– $N = \lceil 1/\epsilon \rceil$." è¿™è¡¨ç¤ºï¼Œå¦‚æœç»™å®š $\epsilon > 0$ï¼Œæˆ‘ä»¬æƒ³è®© $|f_n(x) - x| < \epsilon$ å¯¹æ‰€æœ‰ $x \in (0, 1)$ æˆç«‹ï¼Œåªéœ€è¦å– $n \ge N = \lceil 1/\epsilon \rceil$. å› ä¸ºå½“ $n \ge N$ æ—¶ï¼Œ$\frac{1}{n} \le \frac{1}{N} \le \epsilon$, ä»è€Œ $|f_n(x) - x| \le \frac{1}{n} \le \epsilon$ å¯¹æ‰€æœ‰ $x \in (0, 1)$ æˆç«‹ã€‚

**åœ¨ (D) ä¸­ï¼Œæé™å‡½æ•°æ˜¯ $x \to 0$ï¼Œä½†å¯¹äº $n=2, 3, \ldots$ï¼Œæˆ‘ä»¬æœ‰ $f_n(1/\sqrt{n}) = 1/e$ for $n=2, 3, \ldots$ï¼Œè¿™è¡¨æ˜å¯¹äº $\epsilon = 1/e$ ä¸å¯èƒ½æ‰¾åˆ°ä¸€è‡´å“åº”ã€‚**

* **è¯¦ç»†è§£é‡Š (D):**
    * **é€ç‚¹æé™:** å¯¹äºä»»æ„ $x \in (0, 1)$ï¼Œ$\lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} e^{-nx^2} = 0$ (å› ä¸º $x^2 > 0$ å¯¹äº $x \in (0, 1)$ï¼Œæ‰€ä»¥ $-nx^2 \to -\infty$ ä¸” $e^{-nx^2} \to 0$)ã€‚ é€ç‚¹æé™å‡½æ•° $f(x) = 0$ã€‚
    * **éä¸€è‡´æ”¶æ•›:** éœ€è¦æŸ¥çœ‹ $\sup_{x \in (0, 1)} |f_n(x) - f(x)| = \sup_{x \in (0, 1)} |e^{-nx^2}| = \sup_{x \in (0, 1)} e^{-nx^2}$ æ˜¯å¦è¶‹å‘äº 0 å½“ $n \to \infty$ã€‚
    * å½“ $x \to 0^+$ æ—¶ï¼Œ$-nx^2 \to 0$ï¼Œæ‰€ä»¥ $e^{-nx^2} \to e^0 = 1$ã€‚
    * å› æ­¤ï¼Œ$\sup_{x \in (0, 1)} e^{-nx^2} = 1$ (å› ä¸º $e^{-nx^2} < 1$ å¯¹äº $x > 0$, ä½†æ˜¯å¯ä»¥ä»»æ„æ¥è¿‘ 1 å½“ $x \to 0^+$)ã€‚  ä¸€è‡´èŒƒæ•°æ˜¯ 1ï¼Œä¸è¶‹äº 0ï¼Œæ‰€ä»¥éä¸€è‡´æ”¶æ•›ã€‚
    * è§£é‡Šä¸­ç”¨ $x = 1/\sqrt{n}$ ä½œä¸ºä¾‹å­ã€‚å½“ $x = 1/\sqrt{n}$ æ—¶ï¼Œ$f_n(1/\sqrt{n}) = e^{-n(1/\sqrt{n})^2} = e^{-n/n} = e^{-1} = 1/e$ã€‚  è¿™æ„å‘³ç€å­˜åœ¨æŸäº› $x$ å€¼ï¼ˆä¾‹å¦‚ $x = 1/\sqrt{n}$ï¼‰ï¼Œä½¿å¾— $|f_n(x) - 0| = |1/e - 0| = 1/e$ï¼Œæ— è®º $n$ å¤šå¤§ï¼Œè¯¯å·®éƒ½ä¿æŒåœ¨ $1/e$ï¼Œä¸è¶‹è¿‘äº 0ï¼Œæ‰€ä»¥ä¸æ˜¯ä¸€è‡´æ”¶æ•›ã€‚

**åœ¨ (E) ä¸­ï¼Œæé™å‡½æ•°æ˜¯ $x \to 0$ï¼Œä¸” $0 < x/n < 1/n$ï¼Œä»¥åŠ $\lim_{y \downarrow 0} y \ln y = 0$ æš—ç¤ºäº†ä¸€è‡´æ”¶æ•›æ€§ã€‚ï¼ˆå¦‚æœ $\delta > 0$ ä½¿å¾—å½“ $0 < y < \delta$ æ—¶æœ‰ $|y \ln y| < \epsilon$ï¼Œæˆ‘ä»¬å¯ä»¥å– $N = \lceil 1/\delta \rceil$ ä½œä¸ºå¯¹ $\epsilon$ çš„å“åº”ã€‚ï¼‰**

* **è¯¦ç»†è§£é‡Š (E):**
    * **é€ç‚¹æé™:** å¯¹äºä»»æ„ $x \in (0, 1)$ï¼Œ$\lim_{n \to \infty} f_n(x) = \lim_{n \to \infty} \frac{x}{n} \ln \frac{x}{n} = 0$ (ä½¿ç”¨æé™ $\lim_{y \to 0^+} y \ln y = 0$ï¼Œä»¤ $y = x/n$. å½“ $n \to \infty$ æ—¶ï¼Œ$x/n \to 0^+$)ã€‚ é€ç‚¹æé™å‡½æ•° $f(x) = 0$ã€‚
    * **ä¸€è‡´æ”¶æ•›:** éœ€è¦æŸ¥çœ‹ $\sup_{x \in (0, 1)} |f_n(x) - f(x)| = \sup_{x \in (0, 1)} |\frac{x}{n} \ln \frac{x}{n}|$ æ˜¯å¦è¶‹å‘äº 0 å½“ $n \to \infty$ã€‚
    * ä»¤ $y = x/n$. å› ä¸º $x \in (0, 1)$, æ‰€ä»¥ $y = x/n \in (0, 1/n)$. æˆ‘ä»¬è¦è€ƒå¯Ÿ $\sup_{y \in (0, 1/n)} |y \ln y|$.
    * æˆ‘ä»¬çŸ¥é“ $\lim_{y \downarrow 0} y \ln y = 0$ã€‚ å‡½æ•° $g(y) = y \ln y$ åœ¨ $(0, 1/n]$ ä¸Šçš„æœ€å¤§ç»å¯¹å€¼ä¼šéšç€ $1/n \to 0$ è€Œè¶‹äº 0ã€‚  æ›´ç²¾ç¡®çš„åˆ†æï¼Œå¯ä»¥æ±‚å¯¼ $g'(y) = \ln y + 1 = 0$, å¾—åˆ° $y = e^{-1} = 1/e$ æ˜¯ $y \ln y$ çš„æœ€å°å€¼ç‚¹ (åœ¨ $y > 0$ èŒƒå›´å†…)ã€‚ æœ€å°å€¼æ˜¯ $e^{-1} \ln (e^{-1}) = -e^{-1} = -1/e$.  æœ€å¤§å€¼å‡ºç°åœ¨åŒºé—´ç«¯ç‚¹æˆ–è€…è¶‹äºç«¯ç‚¹æé™ã€‚ å½“ $y \to 0^+$ æ—¶ $y \ln y \to 0$. å½“ $y = 1/n$ æ—¶ï¼Œ$y \ln y = \frac{1}{n} \ln \frac{1}{n} = -\frac{\ln n}{n}$ã€‚ æ‰€ä»¥ $\sup_{y \in (0, 1/n)} |y \ln y| = \sup_{y \in (0, 1/n)} -y \ln y$ (å› ä¸º $y \ln y \le 0$ å¯¹äº $0 < y \le 1$).  å¯¹äº $n \ge 3$ (å› ä¸º $1/n \le 1/3 < 1/e \approx 0.368$), å‡½æ•° $-y \ln y$ åœ¨ $(0, 1/n]$ ä¸Šæ˜¯é€’å‡çš„ï¼Œæ‰€ä»¥æœ€å¤§å€¼åœ¨ $y \to 0^+$ å¤„æ¥è¿‘ 0ï¼Œæˆ–è€…åœ¨ $y = 1/n$ å¤„ä¸º $\frac{\ln n}{n}$. å› æ­¤ï¼Œ$\sup_{x \in (0, 1)} |\frac{x}{n} \ln \frac{x}{n}| = \sup_{y \in (0, 1/n)} |y \ln y| = \max \{ 0, |\frac{\ln n}{n}| \} = \frac{\ln n}{n}$ (å¯¹äº $n \ge 1$).
    * ç”±äº $\lim_{n \to \infty} \frac{\ln n}{n} = 0$ï¼Œæ‰€ä»¥ä¸€è‡´èŒƒæ•°è¶‹äº 0ï¼Œå› æ­¤å‡½æ•°åºåˆ—ä¸€è‡´æ”¶æ•›ã€‚
    * è§£é‡Šä¸­ä¹Ÿæåˆ°äº† $\lim_{y \downarrow 0} y \ln y = 0$ æš—ç¤ºäº†ä¸€è‡´æ”¶æ•›æ€§ã€‚å¹¶è¯´æ˜å¦‚æœé€‰å–åˆé€‚çš„ $\delta$ ä½¿å¾— $0 < y < \delta$ æ—¶ $|y \ln y| < \epsilon$ï¼Œå°±å¯ä»¥æ‰¾åˆ°å¯¹åº”çš„ $N$.

æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š **é€‰é¡¹ (C) å’Œ (E) ä¸€è‡´æ”¶æ•›ã€‚**

## Unique and Existence Theorem 

### Sample Questions

**Question 1:**

Which of the following ODEs has distinct solutions $y_1, y_2: \mathbb{R} \to \mathbb{R}$ satisfying $y_1(0)=y_2(0)$ and $y_1'(0)=y_2'(0)$?

- [ ] $y'' = |y'|$
- [ ] $y'' = \sqrt{t} \cdot y$
- [x] $y'' = t\sqrt{y}$
- [ ] $y'' = |y|$
- [ ] $y \cdot y'' = 0$

**Question 2:**

Which of the following ODEs has distinct solutions $y_1, y_2: [-1,1] \to \mathbb{R}$ satisfying $y_1(0)=y_2(0)=1$?

- [ ] $y' = \sqrt{y^2+1}$
- [ ] $t^2y'' = y$
- [ ] $y' = \sqrt{|t|} \cdot y$
- [x] $(y')^3 = y$
- [ ] $y' = t|y|$

**Question 3:**

Which of the following ODEs has distinct solutions $y_1, y_2: (0,2) \to \mathbb{R}$ satisfying $y_1(1)=y_2(1)$?

- [ ] $y' = -|y|$
- [ ] $y' = -y^2$
- [ ] $y' = y\sqrt{t}$
- [x] $ty' = y$
- [ ] $y' = ty$

**Question 4:**

Which of the following ODEs has distinct solutions $y_1, y_2: I \to \mathbb{R}$ satisfying $y_1(t_0)=y_2(t_0)$ for some $t_0 \in I$?

- [ ] $y' = y$
- [ ] $y' = |y|$
- [ ] $y' = y^2$
- [x] $y' = \sqrt[3]{y}$
- [ ] $y' = \frac{1}{y}$

### Some easy ways to check uniqueness and existence of solutions

1. **å¿«é€Ÿæ’é™¤çº¿æ€§é€‰é¡¹:**  é¦–å…ˆï¼Œå¿«é€Ÿæ‰«ææ‰€æœ‰é€‰é¡¹ï¼Œæ‰¾å‡º**çº¿æ€§å¾®åˆ†æ–¹ç¨‹ (Linear ODE)** çš„é€‰é¡¹ã€‚çº¿æ€§ODEé€šå¸¸ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢˜ç›®æ²¡æœ‰ç‰¹æ„æŒ‡å‡ºå¥‡å¼‚æƒ…å†µæ—¶ï¼Œå€¾å‘äºæ‹¥æœ‰**å”¯ä¸€è§£**ã€‚æ‰€ä»¥ï¼Œå¦‚æœé€‰é¡¹ä¸­æœ‰æ˜æ˜¾çš„çº¿æ€§ODEï¼Œé€šå¸¸å¯ä»¥**ä¼˜å…ˆæ’é™¤**ï¼Œé™¤éé¢˜ç›®æ˜ç¡®æç¤ºä¸å¯»å¸¸çš„æƒ…å†µã€‚

    *   **å¦‚ä½•è¯†åˆ«çº¿æ€§ ODE:** çº¿æ€§ ODE çš„ç‰¹ç‚¹æ˜¯ï¼Œå› å˜é‡ï¼ˆyåŠå…¶å¯¼æ•°ï¼‰åŠå…¶ç³»æ•°ä¹‹é—´æ˜¯**çº¿æ€§å…³ç³»**ï¼Œå³å„é¡¹éƒ½æ˜¯ $y, y', y'', \ldots, y^{(n)}$  çš„çº¿æ€§ç»„åˆï¼Œç³»æ•°å¯ä»¥æ˜¯å…³äºè‡ªå˜é‡ï¼ˆé€šå¸¸æ˜¯ t æˆ– xï¼‰çš„å‡½æ•°ï¼Œä½†ä¸å¯ä»¥æ˜¯ y çš„å‡½æ•°ã€‚
        *   **é€‰é¡¹ç¤ºä¾‹ (å¯æ’é™¤):** $y'' = \sqrt{t} y$,  $y' = t y$, $t^2 y'' = y$ (å³ä½¿ç³»æ•°ä¾èµ–äº t, ç»“æ„ä¸Šä»ç„¶æ˜¯çº¿æ€§çš„å…³äºyåŠå…¶å¯¼æ•°).

2. **é‡ç‚¹å…³æ³¨éçº¿æ€§é€‰é¡¹ä¸­çš„ â€œéçº¿æ€§é¡¹â€ ç±»å‹:**  åœ¨å‰©ä¸‹çš„éçº¿æ€§é€‰é¡¹ä¸­ï¼Œç€é‡æŸ¥çœ‹ä»¥ä¸‹ç±»å‹çš„éçº¿æ€§é¡¹ï¼Œè¿™äº›æ˜¯å¯¼è‡´è§£çš„**ä¸å”¯ä¸€æ€§**çš„â€œå±é™©ä¿¡å·â€ï¼š

    *   **ç»å¯¹å€¼å‡½æ•°:** å‡ºç°  $|y|, |y'|$ ç­‰é¡¹ (**é«˜æ€€ç–‘åº¦**)
        *   ä¾‹ï¼š$y'' = |y'|$ (é¢˜ç›®ä¸­çš„ä¾‹å­),  $y' = t|y|$ (é¢˜ç›®ä¸­ç¬¬2é¢˜çš„ä¾‹å­), $y' = \sqrt{|y|}$ (æ•™æ¡ˆä¸­çš„ä¾‹å­)

    *   **å› å˜é‡ (y) çš„åˆ†æ•°æ¬¡å¹‚æˆ–æ ¹å¼:**  å‡ºç° $\sqrt{y}, y^{2/3}, y^{1/2}$ ç­‰é¡¹ (**é«˜æ€€ç–‘åº¦**)
        *   ä¾‹ï¼š$y' = y^{2/3}$ (é¢˜ç›®ä¸­çš„ä¾‹å­), $y'' = t\sqrt{y}$ (é¢˜ç›®ä¸­ä¾‹é¢˜)

    *   **å¯¼æ•°çš„éçº¿æ€§å‡½æ•°:**  å‡ºç° $(y')^3$ è¿™ç§å¯¼æ•°æ•´ä½“çš„éçº¿æ€§å‡½æ•° (**æœ€é«˜æ€€ç–‘åº¦**). è¿™ç§éçº¿æ€§é€šå¸¸æ¯”ç›´æ¥åœ¨ $y$ ä¸Šçš„éçº¿æ€§æ›´å®¹æ˜“ç ´åè§£çš„å”¯ä¸€æ€§ã€‚

    *   **åˆ†æ¯ä¸­å‡ºç°å› å˜é‡ y:**  å‡ºç° $y$ åœ¨åˆ†æ¯ä¸­ï¼Œä¾‹å¦‚ $\sqrt{y+1}/y$ï¼Œè¿™ç±»é¡¹å¯èƒ½å¯¼è‡´è§£çš„å­˜åœ¨åŒºé—´å—é™ï¼Œä¹Ÿå¯èƒ½å…³è”åˆ°è§£çš„ä¸å”¯ä¸€æ€§ï¼Œä½†ä¸å¦‚ä»¥ä¸Šä¸¤ç§æ˜ç¡® (**ä¸­ç­‰æ€€ç–‘åº¦**)ã€‚

    *   **å¯¹æ•°å‡½æ•°ä¸­å‡ºç°å› å˜é‡ y:** å‡ºç° $y \ln|y|$ ç­‰é¡¹ (**ä¸­ç­‰æ€€ç–‘åº¦**)ã€‚è¿™ç±»é¡¹é€šå¸¸åœ¨ $y=0$ é™„è¿‘ Lipschitz æ¡ä»¶ä¸æˆç«‹ï¼Œå¯èƒ½å¯¼è‡´éå”¯ä¸€è§£ã€‚


3. **å¯¹æ¯”å’Œé€‰æ‹©ï¼š** å¦‚æœæœ‰å¤šä¸ªéçº¿æ€§é€‰é¡¹éƒ½æ— æ³•è¢«å¿«é€Ÿæ’é™¤ï¼Œåˆ™å¯¹æ¯”è¿™äº›é€‰é¡¹ä¸­â€œéçº¿æ€§é¡¹â€çš„â€œå±é™©ç¨‹åº¦â€ï¼Œä¼˜å…ˆé€‰æ‹©åŒ…å« **ä¼˜å…ˆçº§é«˜** çš„éçº¿æ€§é¡¹çš„é€‰é¡¹ã€‚é€šå¸¸åœ¨è€ƒè¯•é€‰æ‹©é¢˜ä¸­ï¼Œæœ€â€œæ˜æ˜¾â€è¿å Lipschitz æ¡ä»¶ï¼Œæœ€åƒæ•™æ¡ˆä¸­å¼ºè°ƒçš„éå”¯ä¸€æ€§ä¾‹å­çš„é€‰é¡¹ï¼Œæ›´æœ‰å¯èƒ½æ˜¯æ­£ç¡®ç­”æ¡ˆã€‚

4. **å›é¡¾æ•™æ¡ˆä¾‹é¢˜ï¼š**  å¿«é€Ÿå›å¿†æ•™æ¡ˆä¸­å¼ºè°ƒçš„éå”¯ä¸€æ€§è§£çš„ä¾‹å­ï¼Œçœ‹å“ªä¸ªé€‰é¡¹çš„éçº¿æ€§å½¢å¼ä¸è¿™äº›ä¾‹é¢˜æœ€ç›¸ä¼¼ã€‚æ•™æ¡ˆå¤šæ¬¡æåˆ° $y'=\sqrt{|y|}$ çš„éå”¯ä¸€è§£ï¼Œä»¥åŠå«æœ‰ç»å¯¹å€¼é¡¹ $|y'|, |y|$ å®¹æ˜“å‡ºç°éå”¯ä¸€è§£ã€‚

5. **æœ€ç»ˆåˆ¤æ–­å¹¶æ ‡æ³¨:**  åœ¨å¿«é€Ÿæ’é™¤çº¿æ€§é€‰é¡¹åï¼Œæ ¹æ®éçº¿æ€§é¡¹çš„ç±»å‹å’Œä¸æ•™æ¡ˆä¾‹é¢˜çš„ç›¸ä¼¼åº¦ï¼Œé€‰æ‹©æœ€æœ‰å¯èƒ½å‡ºç°éå”¯ä¸€è§£çš„é€‰é¡¹ã€‚ å¦‚æœä½ ä»ç„¶ä¸ç¡®å®šï¼Œ**ç»¿è‰²é«˜äº®**çš„é€‰é¡¹å¾€å¾€æ˜¯ä¸€ä¸ªå¼ºçƒˆçš„æç¤ºï¼Œè¡¨æ˜è¯¥é€‰é¡¹æ˜¯â€œé¢„æœŸâ€çš„æ­£ç¡®ç­”æ¡ˆã€‚

### Theorems

**1. Lipschitz Condition**

The lecture notes define the Lipschitz condition in two ways:

*   **Lipschitz condition with respect to  *y*:**

    *   This condition applies to a map $f: D \to \mathbb{R}^n$, where $D \subseteq \mathbb{R} \times \mathbb{R}^n$. We say that $f = f(t, y)$ satisfies a Lipschitz condition with respect to $y$ if we can find a constant $L$ (called the Lipschitz constant, $L \ge 0$) so that for any two points $(t, \mathbf{y}_1)$ and $(t, \mathbf{y}_2)$ within the domain $D$, the following inequality holds:

        $$|f(t, \mathbf{y}_1) - f(t, \mathbf{y}_2)| \le L |\mathbf{y}_1 - \mathbf{y}_2|$$

    *   **In simpler terms:**  For a fixed time $t$, the change in the function value $f(t, y)$ is "controlled" or limited by the change in $y$. The constant $L$ quantifies this control.  If you move from $y_1$ to $y_2$, the function value doesn't "jump" too wildly, the rate of change is bounded.

*   **Locally Lipschitz condition with respect to *y*:**

    *   This is a weaker, more flexible condition. A function $f$ satisfies a *locally* Lipschitz condition with respect to $y$ if, around *every* point $(t, \mathbf{y}) \in D$, you can find a small "neighborhood" $D' \subseteq D$ where the Lipschitz condition is satisfied (though the Lipschitz constant $L$ might change from neighborhood to neighborhood).

    *   **In simpler terms:**  Around every point in the domain, there's a region where the function behaves "nicely" with respect to changes in $y$, meaning the Lipschitz condition (as described above) holds in that local region, even if it doesn't hold over the entire domain $D$.

**The Proposition on Lipschitz Condition:**

The lecture notes state a very important proposition:

> Suppose $D \subseteq \mathbb{R} \times \mathbb{R}^n$ is an open set and $f: D \to \mathbb{R}^n$ has continuous (as (n+1)-variable functions!) partial derivatives with respect to the variables $\mathbf{y} = (y_1, \ldots, y_n)$. Then $f$ satisfies locally a Lipschitz condition with respect to $\mathbf{y}$.

*   **In simpler terms:**  If the function $f(t, y)$ is "smooth enough" in terms of its derivatives with respect to the $y$ variables (partial derivatives exist and are continuous), then it automatically satisfies the locally Lipschitz condition. This is a very useful and practical result because it provides a readily checkable condition (continuous partial derivatives) to ensure local Lipschitz continuity.


**2. Uniqueness Theorem (Ordinary Differential Equations)**

The Uniqueness Theorem stated in the notes is:

> Suppose $D \subseteq \mathbb{R} \times \mathbb{R}^n$ is open and that $f: D \to \mathbb{R}^n$ is continuous and satisfies locally a Lipschitz condition with respect to **y**. If $\phi, \psi: I \to \mathbb{R}^n$ are solutions of an IVP
> $$ \mathbf{y}' = f(t, \mathbf{y}) \wedge \mathbf{y}(t_0) = \mathbf{y}_0, \quad (t_0, \mathbf{y}_0) \in D,$$
> then $\phi(t) = \psi(t)$ for all $t \in I$.

*   **In simpler terms:** If you have a first-order ODE system $\mathbf{y}' = f(t, \mathbf{y})$ where $f$ is "nice" (continuous and locally Lipschitz with respect to $\mathbf{y}$), then for a given initial condition $\mathbf{y}(t_0) = \mathbf{y}_0$, there is **at most one** solution curve that passes through the point $(t_0, \mathbf{y}_0)$. This means if you find one solution, that's the *only* one.


**3. Picard-LindelÃ¶f Theorem (Existence and Uniqueness Theorem)**

The Picard-LindelÃ¶f Theorem stated in the notes is:

> Suppose $D \subseteq \mathbb{R} \times \mathbb{R}^n$ is open and $f: D \to \mathbb{R}^n$, $(t, \mathbf{y}) \to f(t, \mathbf{y})$ is a continuous function which satisfies on $D$ locally a Lipschitz condition with respect to $\mathbf{y}$. Then for every $(t_0, \mathbf{y}_0) \in D$ there exists an interval $I$ containing $t_0$ as an inner point and a solution $\phi: I \to \mathbb{R}^n$ of the IVP $\mathbf{y}' = f(t, \mathbf{y}) \wedge \mathbf{y}(t_0) = \mathbf{y}_0$.

*   **In simpler terms:** Under the same "niceness" conditions on $f$ (continuity and locally Lipschitz w.r.t $\mathbf{y}$), for *any* starting point $(t_0, \mathbf{y}_0)$ in the domain $D$, there is **at least one** solution to the initial value problem. The interval of existence $I$ around $t_0$ is guaranteed, though it might not be the entire real line.

---
#### åˆæ˜¯ç¨€å¥‡å¤æ€ªçš„ç¢ç¢å¿µï¼š
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨ç”»ä¸€æ¡æ›²çº¿ï¼Œè¿™æ¡æ›²çº¿çš„æ–œç‡ç”±ä¸€ä¸ªè§„åˆ™ï¼ˆå¾®åˆ†æ–¹ç¨‹ï¼‰æ§åˆ¶ã€‚

**Lipschitz æ¡ä»¶:** å°±åƒäº¤é€šè§„åˆ™ä¸€æ ·ï¼Œä¿è¯ä½ ç”»çš„æ›²çº¿â€œå¹³æ»‘â€ã€‚å®ƒè¯´çš„æ˜¯ï¼Œè§„åˆ™ä¸èƒ½å¤ªâ€œé‡â€ï¼Œä¸èƒ½å¯¹å¾ˆå°çš„ä½ç½®å˜åŒ–ï¼Œäº§ç”Ÿâ€œè·³è·ƒå¼â€çš„å¤§å¹…åº¦çš„æ–œç‡å˜åŒ–ã€‚**å±€éƒ¨Lipschitzæ¡ä»¶**å°±åƒè¯´ï¼Œåœ¨ä»»ä½•ä¸€å°å—åœ°æ–¹ï¼Œè¿™ä¸ªè§„åˆ™éƒ½æ˜¯â€œå¹³æ»‘â€çš„ï¼Œå³ä½¿æ”¾åˆ°å…¨å±€ï¼Œè§„åˆ™å¯èƒ½ä¼šå˜å¾—æ¯”è¾ƒå¤æ‚ã€‚

**å”¯ä¸€æ€§å®šç†:** æœ‰äº†â€œå¹³æ»‘â€çš„è§„åˆ™ï¼Œå°±åƒäº¤é€šç§©åºè‰¯å¥½ï¼Œé‚£ä¹ˆå¦‚æœä½ æŒ‡å®šäº†æ›²çº¿çš„èµ·ç‚¹ï¼ˆåˆå§‹æ¡ä»¶ï¼‰ï¼Œä½ **æœ€å¤šåªèƒ½ç”»å‡ºä¸€æ¡æ›²çº¿**ã€‚å°±å¥½æ¯”è¯´ï¼Œç»™å®šèµ·ç‚¹ï¼ŒæŒ‰ç…§å¹³æ»‘çš„è§„åˆ™ï¼Œè·¯å¾„å°±å”¯ä¸€ç¡®å®šäº†ï¼Œä¸ä¼šå‡ºç°åˆ†å‰ã€‚

**å­˜åœ¨æ€§å®šç†ï¼ˆPicard-LindelÃ¶f å®šç†):**  åŒæ ·æ˜¯â€œå¹³æ»‘â€çš„è§„åˆ™ï¼Œä¿è¯äº†å¦‚æœä½ æŒ‡å®šäº†ä»»æ„ä¸€ä¸ªèµ·ç‚¹ï¼Œ**è‡³å°‘èƒ½ç”»å‡ºä¸€æ¡æ›²çº¿**ã€‚å°±åƒæ˜¯è¯´ï¼Œåªè¦è§„åˆ™å¹³æ»‘ï¼Œä»ä»»ä½•åœ°ç‚¹å‡ºå‘ï¼Œéƒ½èƒ½æ‰¾åˆ°ä¸€æ¡åˆæ³•çš„è·¯å¾„ã€‚

**æ€»ç»“æˆå¤§ç™½è¯:**

æƒ³è±¡ä½ è¦è§£ä¸€é“è¿·å®«é¢˜ï¼š

* **å¾®åˆ†æ–¹ç¨‹ = è¿·å®«çš„è§„åˆ™:**  è§„åˆ™å‘Šè¯‰ä½ æ¯ä¸€æ­¥çš„æ–¹å‘ï¼ˆæ–œç‡ï¼‰ã€‚
* **Lipschitz æ¡ä»¶ = å¹³æ»‘çš„è§„åˆ™:** ä¿è¯è§„åˆ™ä¸ä¼šå¤ªæ··ä¹±ï¼Œæ–¹å‘ä¸ä¼šçªç„¶è·³å˜ã€‚
* **åˆå§‹æ¡ä»¶ = è¿·å®«çš„èµ·ç‚¹:** ä½ ä»å“ªé‡Œå¼€å§‹èµ°ã€‚

**å®šç†ä»¬å‘Šè¯‰ä½ ï¼š**

* **å­˜åœ¨æ€§å®šç† (Picard-LindelÃ¶f):**  åªè¦è§„åˆ™å¤Ÿå¹³æ»‘ï¼ˆ Lipschitz æ¡ä»¶ï¼‰ï¼Œ è¿·å®« **ä¸€å®šæœ‰è·¯å¯èµ°** (è‡³å°‘åœ¨èµ·ç‚¹é™„è¿‘)ã€‚
* **å”¯ä¸€æ€§å®šç†:**  å¦‚æœè§„åˆ™å¾ˆå¹³æ»‘ï¼ˆ Lipschitz æ¡ä»¶ï¼‰ï¼Œ ä»åŒä¸€ä¸ªèµ·ç‚¹å‡ºå‘ï¼Œ **åªèƒ½æœ‰ä¸€æ¡è·¯** èƒ½èµ°é€šã€‚

**ä¸ºä»€ä¹ˆ Lipschitz æ¡ä»¶å¾ˆé‡è¦ï¼Ÿ**

å› ä¸ºå¦‚æœè§„åˆ™ä¸å¤Ÿå¹³æ»‘ï¼ˆä¸æ»¡è¶³ Lipschitz æ¡ä»¶ï¼‰ï¼Œå°±å¥½åƒè¿·å®«çš„è·¯çªç„¶æ–­æ‰ï¼Œæˆ–è€…å‡ºç°å²”è·¯å£ï¼Œå¯¼è‡´ï¼š

* **æ²¡æœ‰è·¯å¯èµ° (è§£ä¸å­˜åœ¨):**  ä»æŸä¸ªèµ·ç‚¹å‡ºå‘ï¼Œå¯èƒ½æ ¹æœ¬æ‰¾ä¸åˆ°ç¬¦åˆè§„åˆ™çš„è·¯å¾„ã€‚
* **æœ‰å¤šæ¡è·¯å¯èµ° (è§£ä¸å”¯ä¸€):**  ä»åŒä¸€ä¸ªèµ·ç‚¹å‡ºå‘ï¼Œå¯èƒ½æœ‰ä¸æ­¢ä¸€æ¡è·¯å¾„ç¬¦åˆè§„åˆ™ã€‚

**ç®€è€Œè¨€ä¹‹ï¼Œ Lipschitz æ¡ä»¶å°±åƒæ˜¯ç»™å¾®åˆ†æ–¹ç¨‹çš„è§£ä¸Šäº†ä¸€æŠŠâ€œå®‰å…¨é”â€ï¼Œç¡®ä¿äº†è§£çš„å­˜åœ¨å’Œå”¯ä¸€æ€§ã€‚**

---

### How to solve the problem

Let's break down the explanation for **Question 1:** Which of the following ODE's has distinct solutions $y_1, y_2: \mathbb{R} \to \mathbb{R}$ satisfying $y_1(0) = y_2(0) = 1$?", where the correct answer was identified as  $y' = y^{2/3}$.

**Explanation from the Provided Text:**

The text focuses on **Option 1: $y' = y^{2/3}$**

*   **Non-Lipschitz Continuous Function:** The function $f(y) = y^{2/3}$ in this ODE is highlighted as not satisfying the Lipschitz condition. The derivative $f'(y) = \frac{2}{3}y^{-1/3}$ becomes unbounded as $y$ approaches 0. This failure of the Lipschitz condition is the root cause of the non-uniqueness.

*   **Comparison to Example in Lecture Notes:**  The text points out that $y' = y^{2/3}$ "behaves like $y' = \sqrt{|y|}$, which we have discussed in the lecture". As we saw from page 28 of theæ•™æ¡ˆ,  $y' = \sqrt{|y|}$  (example 7) indeed demonstrates non-unique solutions, providing an analogical basis to expect non-uniqueness for $y' = y^{2/3}$.

*   **Explicit Non-Unique Solutions Provided:**  The solution goes further and gives *explicit* examples of distinct solutions that satisfy the *same* initial conditions $y(0) = y'(0) = 1$ (although in the question, it's only asking for $y(0)=1$, the provided text uses slightly different initial condition to demonstrate non-uniqueness in broader sense which is sufficient for understanding the core idea):
    *   **Solution 1:** $y_1(t) = \frac{1}{27}t^3$.  It is confirmed that $y'_1(0) = 0$. However, this seems to contradict the problem statement which specifies $y_1(0) = y_2(0) = 1$. It seems there's slight mismatch in the problem setting vs example details but the core idea is still illustrated correctly in the explanation regarding non-uniqueness and lack of Lipschitz condition for $y^{2/3}$. We should likely consider  $y_1(0)=1$ part of the question incorrect for this particular example based on explanation.

    *   **Solution 2:** $y_2(t) = \begin{cases} \frac{1}{27}(t)^3 & \text{if } t \ge 0 \\ 0 & \text{if } t < 0 \end{cases}$. This piecewise-defined function is also presented as a solution. It is stated both $y_1(t)$ and $y_2(t)$ satisfy initial condition, and hence demonstrates non-uniqueness of the IVP. The crucial detail is both of them have same $y(3)=y_1(3)=y_2(3)$ to indirectly link them for same initial condition point. Again the exact point of matching initial condition seems slightly off but idea of non-uniqueness is clearly illustrated.

*   **Why Existence and Uniqueness Theorem (EUT) Doesn't Apply:** The explanation explicitly mentions, "The EUT doesn't apply, since the derivative of $y \to y^{2/3}$ is unbounded near $y = 0$." This is the direct link back to the core theory from the lecture notes.


**Explanation for Other Options (why they are *not* chosen):**

The explanation states for "The other 4 ODE's either satisfy the assumptions of the EUT globally ($y' = \tan y$ and $y' = y \ln |y|$)," or have "no solutions with $y(0) = 1$ ($ty' = y$), or have non-uniqueness only at points that a solution with the given initial condition cannot reach ($y' = \sqrt{y^2+1}/y$)." Let's clarify these:

*   **$y' = \tan y$ and $y' = y \ln |y|$ (Stated as EUT applying globally, which is likely incorrect or typo):** In the quick analysis we had marked  $y' = \tan y$ and $y' = y \ln |y|$ as potential candidates for non-uniqueness (with "medium suspicion").  The provided explanation here stating that they *satisfy* EUT globally seems to be **incorrect or a typo**.  We identified $y \ln |y|$ as non-Lipschitz around y=0. For $\tan y$,  although it *can* be Lipschitz locally (away from asymptotes), its periodic nature and the nature of tangent function's growth may not globally meet the exact condition globally to guarantee global uniqueness without further investigation, though *locally* they may. This part of the provided answer explanation seems inaccurate or requires more nuance to be precise and possibly contradictory to the previous heuristic reasoning about non-uniqueness candidates.  In typical introductory ODE contexts though, if an option *can* exhibit non-uniqueness in certain circumstances they are often considered "less unique" in general selection.  Therefore while not perfectly Lipschitz continuous globally in y-space, their deviation from the most suspect non-Lipschitz term ($y^{2/3}$) makes them less likely choices given other more prominent non-uniqueness candidate ODE like $y' = y^{2/3}$.

*   **$ty' = y$ (No solution with y(0)=1):** For the linear ODE $ty' = y$, rearranging to $y' = \frac{1}{t}y$, we see that at $t = 0$, the coefficient $\frac{1}{t}$ is undefined. This means the Existence and Uniqueness theorem may not directly apply at $t = 0$. Further if we assume $y(t) = x$ to be constant solution in equation $ty' = y$, then we need $0 = x$ for any constant x, i.e. $x = 0$, hence only $y(t) = 0$ could be a constant solution which doesn't satisfy initial condition $y(0) = 1$. This explains why for  $ty' = y$ it says it has "no solutions with $y(0)=1$". In fact, consider the equation as linear form $y' - \frac{1}{t}y = 0$.  Integrating factor method leads to $\mu(t) = e^{\int -\frac{1}{t} dt} = e^{-\ln|t|} = \frac{1}{|t|}$. If $t>0$,  $\mu(t) = \frac{1}{t}$, $\frac{d}{dt}(\frac{1}{t} y) = 0$, hence $\frac{1}{t} y = C$, $y(t) = Ct$, and $y(0) = 0$. For $t<0$, same idea leads to $y(t) = C't$, again $y(0) = 0$. For $y(0) = 1$ condition it won't have solution based on these form of solutions around t=0 and therefore justifies reason it's not the intended non-uniqueness case for the initial condition question type, which should have solutions but non-unique ones.

*   **$y' = \sqrt{y^2+1}/y$ (Non-uniqueness away from given condition point):**  For $y' = \sqrt{y^2+1}/y$, the explanation mentions non-uniqueness might "only be at points that a solution with the given initial condition cannot reach". The term $y$ in the denominator $\sqrt{y^2+1}/y$ hints at issues as $y \to 0$. For initial condition $y(0) = 1 \ne 0$, solutions start at $y \ne 0$ and stay away, making the local issue around $y=0$ not directly relevant to question.

## Orthogonal Trajectories 

### **Methodology for Orthogonal Trajectories Problems:**

To find the orthogonal trajectories of a given family of curves, follow these steps:

1.  **Find the Differential Equation (ODE) of the Given Family:**
    *   Start with the equation describing the given family of curves, usually involving a parameter (like `C`).
    *   Differentiate the equation implicitly with respect to `x`, treating `y` as a function of `x`.
    *   Eliminate the parameter `C` from the differentiated equation and the original family equation to obtain a first-order ODE of the form  $f(x, y, y') = 0$ or ideally in explicit form $y' = F(x, y)$. This ODE describes the slope of the tangent line to any curve in the given family at a point (x, y).

2.  **Determine the ODE for Orthogonal Trajectories:**
    *   The slopes of orthogonal trajectories are *negative reciprocals* of the slopes of the original family.
    *   If the ODE of the original family is $y' = F(x, y)$, replace $y'$ with $-1/y'$ to get the ODE for the orthogonal trajectories: $-1/y' = F(x, y)$ which is often rewritten as $y' = -1/F(x, y)$.
    *   Alternatively, if the ODE of the original family is in differential form $M(x, y) dx + N(x, y) dy = 0$ (derived from $F(x, y) = C$), the ODE for orthogonal trajectories is given by $-N(x, y) dx + M(x, y) dy = 0$. This comes from swapping the coefficients and negating one, corresponding to switching $(M, N)$ to $(-N, M)$ or $(N, -M)$ vectors orthogonal to original gradient vectors.

3.  **Solve the ODE for Orthogonal Trajectories:**
    *   Solve the new ODE obtained in step 2 using appropriate techniques (e.g., separable equations, exact equations, integrating factors, etc.).
    *   The solution will represent the family of orthogonal trajectories.

4.  **Present the Result:**
    *   Express the solution as a family of curves, ideally in implicit form $G(x, y) = C$ or explicit form $y = H(x, C)$ where C is a parameter.

### **Example**

**Example 1: Orthogonal Trajectories of Circles Through (1, 0) and (-1, 0)**

*   **Family of Circles Equation (Implicit):** $x^2 + (y - C)^2 = C^2 + 1$  or $x^2 + y^2 = 2Cy$.
*   **Deriving ODE (Step 1 from notes):**
    *   Rewrite $F(x, y) = \frac{x^2+y^2-1}{2y} = C$.
    *   Compute partial derivatives for differential form $f_x dx + f_y dy = 0$:
        *   $f_x = \frac{x}{y}$
        *   $f_y = \frac{y^2 - x^2 - 1}{2y^2}$
        *   ODE is $f_x dx + f_y dy = \frac{x}{y} dx + \frac{y^2 - x^2 - 1}{2y^2} dy = 0$.

*   **ODE for Orthogonal Trajectories (Step 2 from notes):**
    *   Replace $f_x dx + f_y dy = 0$ with  $-f_y dx + f_x dy = 0$:
        *   $-f_y dx + f_x dy = -(\frac{y^2 - x^2 - 1}{2y^2}) dx + \frac{x}{y} dy = 0$.

*   **Simplifying and Solving ODE (Step 3 from notes):**
    *   Clear denominators: $(x^2 - y^2 - 1) dx + 2xy dy = 0$.
    *   This ODE needs to be solved (using methods from prior lectures, but in example just shown to be the orthogonal ODE without solving it explicitly).

**Example 2: Orthogonal Trajectories of  $y = Ce^{-x}$**

*   **Given Family Equation (Explicit):** $y = Ce^{-x}$
*   **Deriving ODE (Step 1 from notes):**
    *   Rewrite as $ye^x = C$.
    *   Differentiate implicitly: $d(ye^x) = (y'e^x + ye^x)dx = 0 \implies y'e^x + ye^x = 0 \implies y' + y = 0$.

*   **ODE for Orthogonal Trajectories (Step 2 from notes):**
    *   For ODE $y' + y = 0$, corresponding to $-e^{-x} y dx + e^{-x} dy = 0$.
    *   Orthogonal trajectories ODE: $-e^{-x} dx + (-e^{-x}y) dy = 0  \implies dx + y dy = 0$.

*   **Solving Orthogonal ODE (Step 3 from notes):**
    *   Solve $dx + y dy = 0$, which is exact and separable.
    *   Integrate directly to get $x + \frac{1}{2}y^2 = C \implies 2x + y^2 = C \implies 2x + y^2 = C$.

## Define a contraction

### **Definition of a Contraction Mapping:**

To determine if a function  `T(x)` is a contraction on a given interval (e.g., [1, 2]), you need to verify **two key conditions**:

1.  **Self-Mapping (Interval Preservation):**
    *   **Test Endpoints:** Check the function's value at the interval's endpoints. For $[a, b]$, evaluate $T(a)$ and $T(b)$.
    *   **Check Range:** For all $x$ within the interval $[a, b]$, verify that $T(x)$ also falls within or on the boundary of $[a, b]$. If the function is monotonic (increasing or decreasing) on the interval, checking the endpoints might be enough. For more complex functions like quadratics, consider the function's vertex or other critical points within interval and examine if the range remains within interval.
    *   **Elimination Criterion:** If a function FAILS to map the interval into itself (i.e., there exists at least one $x \in [a, b]$ where $T(x) \notin [a, b]$), it is **not** a contraction, and you can eliminate this option.

2.  **Distance Contraction (Using the Derivative & Mean Value Theorem):**
    *   **Find the Derivative:** Calculate the derivative $T'(x)$ of the function.
    *   **Bound the Derivative's Absolute Value:** Find the maximum absolute value of the derivative $|T'(x)|$ on the interval $[a, b]$. Let's call this maximum value *C*. For quadratic function check the endpoints and vertex for maximum absolute derivative.
    *   **Contraction Constant Check:** If the maximum absolute value C is strictly less than 1 ($C < 1$), then the function is a contraction mapping on the interval [a, b], with *C* as the contraction constant.  If $C \ge 1$ or you cannot guarantee $C < 1$ uniformly on the interval, the function is **not** a contraction on the entire interval (even though it *might* still reduce distance locally, but not uniformly with a constant less than 1).

### **Example**

Let's apply this method to option **(D) $x \mapsto T(x) = (x^2 + 5)/6$ for interval [1, 2]**, which is the correct answer according to the provided text.

1.  **Self-Mapping Check:**
    *   $T(1) = (1^2 + 5)/6 = 1$ (Lower endpoint maps to lower endpoint)
    *   $T(2) = (2^2 + 5)/6 = 9/6 = 3/2$ (Upper endpoint maps inside interval [1, 2])
    *   The derivative $T'(x) = x/3$ is positive on $[1, 2]$ indicating $T(x)$ is increasing function on $[1, 2]$.
    *   Since $T(x)$ is increasing and maps endpoints to values within [1, 2] and is continuous, by Intermediate Value Theorem (or direct observation of function), all values $T(x)$ for $x \in [1, 2]$ will be between $T(1)=1$ and $T(2)=3/2$, thus ensuring that $T(x) \in [1, 2]$ for all $x \in [1, 2]$.  **Self-mapping condition is satisfied.**

2.  **Distance Contraction Check:**
    *   Derivative $T'(x) = x/3$.
    *   Maximum absolute value of $T'(x)$ on $[1, 2]$ occurs at $x = 2$, giving $|T'(2)| = |2/3| = 2/3$.
    *   Since $C = 2/3 < 1$, the **distance contraction condition is satisfied**.

**Conclusion for Option (D):**  Option (D) passes both conditions, confirming it is a contraction mapping on [1, 2].

**Why Other Options Fail (from the provided text):**

*   **Options (A) and (B):**  Derivative $T'(x)$ for option (B) is  $T'(x) = -3 + 2x$. At $x=2$, $T'(2) = 1$. The magnitude of derivative can be *too close* to 1 or exceed 1 on interval, and it is *not possible* to find uniform $C < 1$ bound for derivative magnitude over the whole interval, violating the contraction distance condition. Although for option (A) no specific derivative analysis is performed but same principle of derivative test likely also leads to failing contraction condition.

*   **Options (C) and (E):** For Option (E), the solution directly checks $T(1) = 1/(2 \cdot 1) = 1/2$ which is outside of the interval $[1, 2]$.  This violates the **self-mapping condition**. The option C is grouped with E failing on self-mapping for similar reasons at endpoint or within the interval, even though no explicit computation for C is done.

---
ä»¥ä¸‹å“ªä¸ªå‡½æ•°å®šä¹‰äº†åŒºé—´ [1, 2] ä¸Šçš„å‹ç¼©æ˜ å°„ï¼Ÿ

åœ¨ä¸‹åˆ—ç»™å‡ºçš„å‡½æ•°ä¸­ï¼Œå“ªä¸€ä¸ªå‡½æ•°å…·æœ‰â€œå‹ç¼©â€åŒºé—´ [1, 2] çš„ç‰¹æ€§ï¼Œå³ï¼š

1. **è‡ªæ˜ å°„:** å¯¹äºåŒºé—´ [1, 2] å†…çš„ä»»ä½•æ•°å­— `x`ï¼Œåº”ç”¨å‡½æ•° `T(x)` åï¼Œç»“æœä»ç„¶è½å›åŒºé—´ [1, 2] å†…ã€‚ æ¢å¥è¯è¯´ï¼Œå‡½æ•°å°†åŒºé—´ [1, 2] "æ˜ å°„åˆ°è‡ªèº«å†…éƒ¨"ã€‚
2. **è·ç¦»æ”¶ç¼©:**  å­˜åœ¨ä¸€ä¸ªå¸¸æ•° `C` (å°äº 1)ï¼Œä½¿å¾—å¯¹äºåŒºé—´ [1, 2] å†…çš„ä»»æ„ä¸¤ä¸ªæ•°å­— `x` å’Œ `y`ï¼Œå®ƒä»¬åº”ç”¨å‡½æ•°åçš„åƒ `T(x)` å’Œ `T(y)` ä¹‹é—´çš„è·ç¦»ï¼Œæ¯” `x` å’Œ `y` ä¹‹é—´çš„è·ç¦»æ›´å°ã€‚ç®€å•è¯´å°±æ˜¯å‡½æ•°è®©ç‚¹ä¸ç‚¹ä¹‹é—´çš„è·ç¦»â€œæ”¶ç¼©â€äº†ã€‚

**æ€»ç»“ä¸€ä¸‹ï¼Œè¦æˆä¸ºåŒºé—´ [1, 2] ä¸Šçš„å‹ç¼©æ˜ å°„ï¼Œå‡½æ•° T(x) å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š**

*   **è‡ªæ˜ å°„ (ä¿åŒºé—´æ€§):**  æŠŠåŒºé—´ [1, 2] ä¸­çš„ä»»ä½•æ•°å­—ä»ç„¶æ˜ å°„åˆ°åŒºé—´ [1, 2] ä¸­ã€‚
*   **å‹ç¼©è·ç¦»:** ä½¿åŒºé—´å†…ä»»æ„ä¸¤ç‚¹é—´çš„è·ç¦»æŒ‰å°äº 1 çš„æ¯”ä¾‹ç¼©å°ã€‚

**ä¸ºäº†è§£ç­”è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å¯¹æ¯ä¸ªé€‰é¡¹çš„å‡½æ•°ï¼ŒéªŒè¯å…¶åœ¨åŒºé—´ [1, 2] ä¸Šæ˜¯å¦åŒæ—¶æ»¡è¶³è¿™ä¸¤ä¸ªæ¡ä»¶ã€‚ åªæœ‰ä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³çš„å‡½æ•°ï¼Œæ‰èƒ½è¢«è§†ä¸ºåŒºé—´ [1, 2] ä¸Šçš„å‹ç¼©æ˜ å°„ã€‚**

## Did you find this page helpful? Consider sharing it ğŸ™Œ
